{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test and train set\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.710304   \n",
       "1         -0.957686         -0.943068  ...                        -0.861499   \n",
       "2         -0.977469         -0.938692  ...                        -0.760104   \n",
       "3         -0.989302         -0.938692  ...                        -0.482845   \n",
       "4         -0.990441         -0.942469  ...                        -0.699205   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "2                    -0.118559                              0.177899   \n",
       "3                    -0.036788                             -0.012892   \n",
       "4                     0.123320                              0.122542   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "2                          0.100699                              0.808529   \n",
       "3                          0.640011                             -0.485366   \n",
       "4                          0.693578                             -0.615971   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
       "0             -0.841247              0.179941             -0.058627        1   \n",
       "1             -0.844788              0.180289             -0.054317        1   \n",
       "2             -0.848933              0.180637             -0.049118        1   \n",
       "3             -0.848649              0.181935             -0.047663        1   \n",
       "4             -0.847865              0.185151             -0.043892        1   \n",
       "\n",
       "   Activity  \n",
       "0  STANDING  \n",
       "1  STANDING  \n",
       "2  STANDING  \n",
       "3  STANDING  \n",
       "4  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               209\n",
       "STANDING              179\n",
       "LAYING                164\n",
       "WALKING_UPSTAIRS      159\n",
       "WALKING_DOWNSTAIRS    145\n",
       "SITTING               143\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 563), (999, 563))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WALKING               185\n",
       "LAYING                183\n",
       "STANDING              178\n",
       "SITTING               170\n",
       "WALKING_UPSTAIRS      149\n",
       "WALKING_DOWNSTAIRS    134\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'fBodyBodyGyroJerkMag-kurtosis()', 'angle(tBodyAccMean,gravity)',\n",
       "       'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity'],\n",
       "      dtype='object', length=563)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.272522</td>\n",
       "      <td>-0.017315</td>\n",
       "      <td>-0.106699</td>\n",
       "      <td>-0.564767</td>\n",
       "      <td>-0.421911</td>\n",
       "      <td>-0.601705</td>\n",
       "      <td>-0.596136</td>\n",
       "      <td>-0.439798</td>\n",
       "      <td>-0.591796</td>\n",
       "      <td>-0.411679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219581</td>\n",
       "      <td>-0.541213</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>-0.013146</td>\n",
       "      <td>-0.545935</td>\n",
       "      <td>0.058899</td>\n",
       "      <td>-0.033470</td>\n",
       "      <td>2.936937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070183</td>\n",
       "      <td>0.041918</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>0.428018</td>\n",
       "      <td>0.501715</td>\n",
       "      <td>0.340713</td>\n",
       "      <td>0.400180</td>\n",
       "      <td>0.486153</td>\n",
       "      <td>0.351317</td>\n",
       "      <td>0.530312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351241</td>\n",
       "      <td>0.360838</td>\n",
       "      <td>0.349059</td>\n",
       "      <td>0.469766</td>\n",
       "      <td>0.629273</td>\n",
       "      <td>0.481251</td>\n",
       "      <td>0.471809</td>\n",
       "      <td>0.349956</td>\n",
       "      <td>0.168279</td>\n",
       "      <td>1.636577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.361205</td>\n",
       "      <td>-0.684097</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999300</td>\n",
       "      <td>-0.998359</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>-0.999407</td>\n",
       "      <td>-0.998077</td>\n",
       "      <td>-0.999808</td>\n",
       "      <td>-0.971348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944282</td>\n",
       "      <td>-0.999595</td>\n",
       "      <td>-0.939598</td>\n",
       "      <td>-0.976454</td>\n",
       "      <td>-0.995222</td>\n",
       "      <td>-0.994877</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.875487</td>\n",
       "      <td>-0.980143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.258468</td>\n",
       "      <td>-0.025925</td>\n",
       "      <td>-0.122726</td>\n",
       "      <td>-0.990822</td>\n",
       "      <td>-0.968894</td>\n",
       "      <td>-0.973927</td>\n",
       "      <td>-0.992207</td>\n",
       "      <td>-0.970928</td>\n",
       "      <td>-0.972885</td>\n",
       "      <td>-0.933391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476273</td>\n",
       "      <td>-0.815733</td>\n",
       "      <td>-0.144342</td>\n",
       "      <td>-0.316220</td>\n",
       "      <td>-0.512150</td>\n",
       "      <td>-0.392773</td>\n",
       "      <td>-0.795959</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.103119</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277054</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-0.108829</td>\n",
       "      <td>-0.464909</td>\n",
       "      <td>-0.208523</td>\n",
       "      <td>-0.486962</td>\n",
       "      <td>-0.509215</td>\n",
       "      <td>-0.242165</td>\n",
       "      <td>-0.465163</td>\n",
       "      <td>-0.297181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264434</td>\n",
       "      <td>-0.629299</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>-0.717300</td>\n",
       "      <td>0.223164</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.290635</td>\n",
       "      <td>-0.007523</td>\n",
       "      <td>-0.093717</td>\n",
       "      <td>-0.234604</td>\n",
       "      <td>0.045610</td>\n",
       "      <td>-0.314822</td>\n",
       "      <td>-0.289766</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>-0.302503</td>\n",
       "      <td>-0.008596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015262</td>\n",
       "      <td>-0.361251</td>\n",
       "      <td>0.178357</td>\n",
       "      <td>0.332586</td>\n",
       "      <td>0.538985</td>\n",
       "      <td>0.350713</td>\n",
       "      <td>-0.606748</td>\n",
       "      <td>0.281283</td>\n",
       "      <td>0.082679</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.498177</td>\n",
       "      <td>0.324130</td>\n",
       "      <td>0.346658</td>\n",
       "      <td>0.543347</td>\n",
       "      <td>0.532506</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>0.495926</td>\n",
       "      <td>0.502260</td>\n",
       "      <td>0.554965</td>\n",
       "      <td>0.680338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>0.955207</td>\n",
       "      <td>0.998425</td>\n",
       "      <td>0.994519</td>\n",
       "      <td>0.971511</td>\n",
       "      <td>0.799174</td>\n",
       "      <td>0.385117</td>\n",
       "      <td>0.265795</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "count         999.000000         999.000000         999.000000   \n",
       "mean            0.272522          -0.017315          -0.106699   \n",
       "std             0.070183           0.041918           0.056029   \n",
       "min            -0.361205          -0.684097          -1.000000   \n",
       "25%             0.258468          -0.025925          -0.122726   \n",
       "50%             0.277054          -0.017185          -0.108829   \n",
       "75%             0.290635          -0.007523          -0.093717   \n",
       "max             0.498177           0.324130           0.346658   \n",
       "\n",
       "       tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "count        999.000000        999.000000        999.000000        999.000000   \n",
       "mean          -0.564767         -0.421911         -0.601705         -0.596136   \n",
       "std            0.428018          0.501715          0.340713          0.400180   \n",
       "min           -0.999300         -0.998359         -0.999454         -0.999407   \n",
       "25%           -0.990822         -0.968894         -0.973927         -0.992207   \n",
       "50%           -0.464909         -0.208523         -0.486962         -0.509215   \n",
       "75%           -0.234604          0.045610         -0.314822         -0.289766   \n",
       "max            0.543347          0.532506          0.364114          0.495926   \n",
       "\n",
       "       tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  \\\n",
       "count        999.000000        999.000000        999.000000  ...   \n",
       "mean          -0.439798         -0.591796         -0.411679  ...   \n",
       "std            0.486153          0.351317          0.530312  ...   \n",
       "min           -0.998077         -0.999808         -0.971348  ...   \n",
       "25%           -0.970928         -0.972885         -0.933391  ...   \n",
       "50%           -0.242165         -0.465163         -0.297181  ...   \n",
       "75%            0.012828         -0.302503         -0.008596  ...   \n",
       "max            0.502260          0.554965          0.680338  ...   \n",
       "\n",
       "       fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "count                       999.000000                       999.000000   \n",
       "mean                         -0.219581                        -0.541213   \n",
       "std                           0.351241                         0.360838   \n",
       "min                          -0.944282                        -0.999595   \n",
       "25%                          -0.476273                        -0.815733   \n",
       "50%                          -0.264434                        -0.629299   \n",
       "75%                          -0.015262                        -0.361251   \n",
       "max                           0.989538                         0.956845   \n",
       "\n",
       "       angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "count                   999.000000                            999.000000   \n",
       "mean                      0.015125                              0.002728   \n",
       "std                       0.349059                              0.469766   \n",
       "min                      -0.939598                             -0.976454   \n",
       "25%                      -0.144342                             -0.316220   \n",
       "50%                       0.010903                              0.017954   \n",
       "75%                       0.178357                              0.332586   \n",
       "max                       0.955207                              0.998425   \n",
       "\n",
       "       angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "count                        999.000000                            999.000000   \n",
       "mean                           0.004282                             -0.013146   \n",
       "std                            0.629273                              0.481251   \n",
       "min                           -0.995222                             -0.994877   \n",
       "25%                           -0.512150                             -0.392773   \n",
       "50%                            0.012891                             -0.016025   \n",
       "75%                            0.538985                              0.350713   \n",
       "max                            0.994519                              0.971511   \n",
       "\n",
       "       angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "count            999.000000            999.000000            999.000000   \n",
       "mean              -0.545935              0.058899             -0.033470   \n",
       "std                0.471809              0.349956              0.168279   \n",
       "min               -1.000000             -0.875487             -0.980143   \n",
       "25%               -0.795959              0.030145             -0.103119   \n",
       "50%               -0.717300              0.223164              0.030593   \n",
       "75%               -0.606748              0.281283              0.082679   \n",
       "max                0.799174              0.385117              0.265795   \n",
       "\n",
       "          subject  \n",
       "count  999.000000  \n",
       "mean     2.936937  \n",
       "std      1.636577  \n",
       "min      1.000000  \n",
       "25%      1.000000  \n",
       "50%      3.000000  \n",
       "75%      5.000000  \n",
       "max      6.000000  \n",
       "\n",
       "[8 rows x 562 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling data\n",
    "from sklearn.utils import shuffle\n",
    "train= shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating features and labels\n",
    "trainData= train.drop('Activity',axis=1).values\n",
    "trainLabel= train.Activity.values\n",
    "\n",
    "testData= test.drop('Activity', axis=1).values\n",
    "testLabel= test.Activity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder= preprocessing.LabelEncoder()\n",
    "\n",
    "#encoding test labels\n",
    "encoder.fit(testLabel)\n",
    "testLabelE = encoder.transform(testLabel)\n",
    "\n",
    "#encoding train labels\n",
    "encoder.fit(trainLabel)\n",
    "trainLabelE = encoder.transform(trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification models:\n",
    "#NN using MLP\n",
    "#Logistic regression\n",
    "#Random forest classifier\n",
    "#KNN\n",
    "#Decision Tree\n",
    "#Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying supervised neural network using multi-layer-preceptron\n",
    "import sklearn.neural_network as nn\n",
    "mlpSGD  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='sgd' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpADAM  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='adam' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpLBFGS  =  nn.MLPClassifier(hidden_layer_sizes=(90,)  \\\n",
    "                        , max_iter=1000 , alpha=1e-4  \\\n",
    "                        , solver='lbfgs' , verbose=10   \\\n",
    "                        , tol=1e-19 , random_state=1  \\\n",
    "                        , learning_rate_init=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15746987\n",
      "Iteration 2, loss = 1.88679125\n",
      "Iteration 3, loss = 1.67577366\n",
      "Iteration 4, loss = 1.51423530\n",
      "Iteration 5, loss = 1.37085665\n",
      "Iteration 6, loss = 1.28109276\n",
      "Iteration 7, loss = 1.20917939\n",
      "Iteration 8, loss = 1.14541897\n",
      "Iteration 9, loss = 1.09049720\n",
      "Iteration 10, loss = 1.04331541\n",
      "Iteration 11, loss = 0.99793454\n",
      "Iteration 12, loss = 0.96024546\n",
      "Iteration 13, loss = 0.92552840\n",
      "Iteration 14, loss = 0.89472421\n",
      "Iteration 15, loss = 0.86557084\n",
      "Iteration 16, loss = 0.83804583\n",
      "Iteration 17, loss = 0.81399701\n",
      "Iteration 18, loss = 0.79045095\n",
      "Iteration 19, loss = 0.76932584\n",
      "Iteration 20, loss = 0.74834348\n",
      "Iteration 21, loss = 0.72904080\n",
      "Iteration 22, loss = 0.71103201\n",
      "Iteration 23, loss = 0.69425904\n",
      "Iteration 24, loss = 0.67738947\n",
      "Iteration 25, loss = 0.66218247\n",
      "Iteration 26, loss = 0.64787925\n",
      "Iteration 27, loss = 0.63373072\n",
      "Iteration 28, loss = 0.62011816\n",
      "Iteration 29, loss = 0.60679288\n",
      "Iteration 30, loss = 0.59420473\n",
      "Iteration 31, loss = 0.58192226\n",
      "Iteration 32, loss = 0.57091923\n",
      "Iteration 33, loss = 0.55925879\n",
      "Iteration 34, loss = 0.54879547\n",
      "Iteration 35, loss = 0.53845505\n",
      "Iteration 36, loss = 0.52876330\n",
      "Iteration 37, loss = 0.51908753\n",
      "Iteration 38, loss = 0.51017622\n",
      "Iteration 39, loss = 0.50079533\n",
      "Iteration 40, loss = 0.49224095\n",
      "Iteration 41, loss = 0.48456714\n",
      "Iteration 42, loss = 0.47585822\n",
      "Iteration 43, loss = 0.46835746\n",
      "Iteration 44, loss = 0.46053966\n",
      "Iteration 45, loss = 0.45490701\n",
      "Iteration 46, loss = 0.44673121\n",
      "Iteration 47, loss = 0.43977445\n",
      "Iteration 48, loss = 0.43306506\n",
      "Iteration 49, loss = 0.42681746\n",
      "Iteration 50, loss = 0.42048547\n",
      "Iteration 51, loss = 0.41419953\n",
      "Iteration 52, loss = 0.40816382\n",
      "Iteration 53, loss = 0.40244380\n",
      "Iteration 54, loss = 0.39757550\n",
      "Iteration 55, loss = 0.39153173\n",
      "Iteration 56, loss = 0.38639562\n",
      "Iteration 57, loss = 0.38088696\n",
      "Iteration 58, loss = 0.37626909\n",
      "Iteration 59, loss = 0.37104950\n",
      "Iteration 60, loss = 0.36726812\n",
      "Iteration 61, loss = 0.36151407\n",
      "Iteration 62, loss = 0.35783009\n",
      "Iteration 63, loss = 0.35247578\n",
      "Iteration 64, loss = 0.34848435\n",
      "Iteration 65, loss = 0.34402286\n",
      "Iteration 66, loss = 0.34007392\n",
      "Iteration 67, loss = 0.33628147\n",
      "Iteration 68, loss = 0.33240229\n",
      "Iteration 69, loss = 0.32827203\n",
      "Iteration 70, loss = 0.32462770\n",
      "Iteration 71, loss = 0.32094624\n",
      "Iteration 72, loss = 0.31724285\n",
      "Iteration 73, loss = 0.31389462\n",
      "Iteration 74, loss = 0.31040587\n",
      "Iteration 75, loss = 0.30692297\n",
      "Iteration 76, loss = 0.30351140\n",
      "Iteration 77, loss = 0.30019229\n",
      "Iteration 78, loss = 0.29781082\n",
      "Iteration 79, loss = 0.29406714\n",
      "Iteration 80, loss = 0.29328742\n",
      "Iteration 81, loss = 0.28806217\n",
      "Iteration 82, loss = 0.28504599\n",
      "Iteration 83, loss = 0.28199617\n",
      "Iteration 84, loss = 0.27929730\n",
      "Iteration 85, loss = 0.27672406\n",
      "Iteration 86, loss = 0.27365877\n",
      "Iteration 87, loss = 0.27110944\n",
      "Iteration 88, loss = 0.26859106\n",
      "Iteration 89, loss = 0.26668382\n",
      "Iteration 90, loss = 0.26328296\n",
      "Iteration 91, loss = 0.26113000\n",
      "Iteration 92, loss = 0.25866515\n",
      "Iteration 93, loss = 0.25657809\n",
      "Iteration 94, loss = 0.25398905\n",
      "Iteration 95, loss = 0.25142158\n",
      "Iteration 96, loss = 0.24996269\n",
      "Iteration 97, loss = 0.24710923\n",
      "Iteration 98, loss = 0.24488991\n",
      "Iteration 99, loss = 0.24322116\n",
      "Iteration 100, loss = 0.24072384\n",
      "Iteration 101, loss = 0.23856326\n",
      "Iteration 102, loss = 0.23642760\n",
      "Iteration 103, loss = 0.23431564\n",
      "Iteration 104, loss = 0.23243063\n",
      "Iteration 105, loss = 0.23040644\n",
      "Iteration 106, loss = 0.22871461\n",
      "Iteration 107, loss = 0.22640455\n",
      "Iteration 108, loss = 0.22480289\n",
      "Iteration 109, loss = 0.22291357\n",
      "Iteration 110, loss = 0.22144391\n",
      "Iteration 111, loss = 0.21931916\n",
      "Iteration 112, loss = 0.21765455\n",
      "Iteration 113, loss = 0.21587811\n",
      "Iteration 114, loss = 0.21407168\n",
      "Iteration 115, loss = 0.21252672\n",
      "Iteration 116, loss = 0.21069171\n",
      "Iteration 117, loss = 0.20914947\n",
      "Iteration 118, loss = 0.20739404\n",
      "Iteration 119, loss = 0.20618998\n",
      "Iteration 120, loss = 0.20448686\n",
      "Iteration 121, loss = 0.20288744\n",
      "Iteration 122, loss = 0.20138696\n",
      "Iteration 123, loss = 0.19960475\n",
      "Iteration 124, loss = 0.19812112\n",
      "Iteration 125, loss = 0.19708792\n",
      "Iteration 126, loss = 0.19528480\n",
      "Iteration 127, loss = 0.19399408\n",
      "Iteration 128, loss = 0.19241083\n",
      "Iteration 129, loss = 0.19142004\n",
      "Iteration 130, loss = 0.19002723\n",
      "Iteration 131, loss = 0.18821915\n",
      "Iteration 132, loss = 0.18701520\n",
      "Iteration 133, loss = 0.18552669\n",
      "Iteration 134, loss = 0.18434258\n",
      "Iteration 135, loss = 0.18323639\n",
      "Iteration 136, loss = 0.18202992\n",
      "Iteration 137, loss = 0.18101597\n",
      "Iteration 138, loss = 0.17938334\n",
      "Iteration 139, loss = 0.17825897\n",
      "Iteration 140, loss = 0.17699894\n",
      "Iteration 141, loss = 0.17581088\n",
      "Iteration 142, loss = 0.17469572\n",
      "Iteration 143, loss = 0.17333608\n",
      "Iteration 144, loss = 0.17216868\n",
      "Iteration 145, loss = 0.17118066\n",
      "Iteration 146, loss = 0.16988783\n",
      "Iteration 147, loss = 0.16910631\n",
      "Iteration 148, loss = 0.16776637\n",
      "Iteration 149, loss = 0.16688232\n",
      "Iteration 150, loss = 0.16590978\n",
      "Iteration 151, loss = 0.16514990\n",
      "Iteration 152, loss = 0.16378833\n",
      "Iteration 153, loss = 0.16255047\n",
      "Iteration 154, loss = 0.16168007\n",
      "Iteration 155, loss = 0.16096332\n",
      "Iteration 156, loss = 0.15960981\n",
      "Iteration 157, loss = 0.15894803\n",
      "Iteration 158, loss = 0.15770382\n",
      "Iteration 159, loss = 0.15747204\n",
      "Iteration 160, loss = 0.15657792\n",
      "Iteration 161, loss = 0.15470498\n",
      "Iteration 162, loss = 0.15430485\n",
      "Iteration 163, loss = 0.15332017\n",
      "Iteration 164, loss = 0.15220532\n",
      "Iteration 165, loss = 0.15125923\n",
      "Iteration 166, loss = 0.15032263\n",
      "Iteration 167, loss = 0.14941894\n",
      "Iteration 168, loss = 0.14909091\n",
      "Iteration 169, loss = 0.14786378\n",
      "Iteration 170, loss = 0.14695917\n",
      "Iteration 171, loss = 0.14612849\n",
      "Iteration 172, loss = 0.14519224\n",
      "Iteration 173, loss = 0.14457470\n",
      "Iteration 174, loss = 0.14372527\n",
      "Iteration 175, loss = 0.14282559\n",
      "Iteration 176, loss = 0.14205820\n",
      "Iteration 177, loss = 0.14127416\n",
      "Iteration 178, loss = 0.14079406\n",
      "Iteration 179, loss = 0.13976385\n",
      "Iteration 180, loss = 0.13913062\n",
      "Iteration 181, loss = 0.13826983\n",
      "Iteration 182, loss = 0.13749580\n",
      "Iteration 183, loss = 0.13685820\n",
      "Iteration 184, loss = 0.13603952\n",
      "Iteration 185, loss = 0.13526507\n",
      "Iteration 186, loss = 0.13466600\n",
      "Iteration 187, loss = 0.13423669\n",
      "Iteration 188, loss = 0.13328784\n",
      "Iteration 189, loss = 0.13262092\n",
      "Iteration 190, loss = 0.13183023\n",
      "Iteration 191, loss = 0.13106651\n",
      "Iteration 192, loss = 0.13049738\n",
      "Iteration 193, loss = 0.12978901\n",
      "Iteration 194, loss = 0.12930322\n",
      "Iteration 195, loss = 0.12871840\n",
      "Iteration 196, loss = 0.12779074\n",
      "Iteration 197, loss = 0.12711515\n",
      "Iteration 198, loss = 0.12651540\n",
      "Iteration 199, loss = 0.12592851\n",
      "Iteration 200, loss = 0.12544907\n",
      "Iteration 201, loss = 0.12497361\n",
      "Iteration 202, loss = 0.12406426\n",
      "Iteration 203, loss = 0.12347680\n",
      "Iteration 204, loss = 0.12283837\n",
      "Iteration 205, loss = 0.12222934\n",
      "Iteration 206, loss = 0.12177323\n",
      "Iteration 207, loss = 0.12107359\n",
      "Iteration 208, loss = 0.12055246\n",
      "Iteration 209, loss = 0.11985811\n",
      "Iteration 210, loss = 0.11926054\n",
      "Iteration 211, loss = 0.11879823\n",
      "Iteration 212, loss = 0.11813745\n",
      "Iteration 213, loss = 0.11769981\n",
      "Iteration 214, loss = 0.11703153\n",
      "Iteration 215, loss = 0.11685636\n",
      "Iteration 216, loss = 0.11595706\n",
      "Iteration 217, loss = 0.11550393\n",
      "Iteration 218, loss = 0.11508599\n",
      "Iteration 219, loss = 0.11465346\n",
      "Iteration 220, loss = 0.11385388\n",
      "Iteration 221, loss = 0.11369393\n",
      "Iteration 222, loss = 0.11354960\n",
      "Iteration 223, loss = 0.11237257\n",
      "Iteration 224, loss = 0.11189349\n",
      "Iteration 225, loss = 0.11130916\n",
      "Iteration 226, loss = 0.11091205\n",
      "Iteration 227, loss = 0.11038010\n",
      "Iteration 228, loss = 0.10974436\n",
      "Iteration 229, loss = 0.10924980\n",
      "Iteration 230, loss = 0.10883586\n",
      "Iteration 231, loss = 0.10827882\n",
      "Iteration 232, loss = 0.10787926\n",
      "Iteration 233, loss = 0.10734598\n",
      "Iteration 234, loss = 0.10715852\n",
      "Iteration 235, loss = 0.10662230\n",
      "Iteration 236, loss = 0.10618629\n",
      "Iteration 237, loss = 0.10578827\n",
      "Iteration 238, loss = 0.10526099\n",
      "Iteration 239, loss = 0.10514667\n",
      "Iteration 240, loss = 0.10414762\n",
      "Iteration 241, loss = 0.10368508\n",
      "Iteration 242, loss = 0.10326068\n",
      "Iteration 243, loss = 0.10300169\n",
      "Iteration 244, loss = 0.10248000\n",
      "Iteration 245, loss = 0.10203536\n",
      "Iteration 246, loss = 0.10151285\n",
      "Iteration 247, loss = 0.10123910\n",
      "Iteration 248, loss = 0.10076906\n",
      "Iteration 249, loss = 0.10021099\n",
      "Iteration 250, loss = 0.09996596\n",
      "Iteration 251, loss = 0.09944817\n",
      "Iteration 252, loss = 0.09897694\n",
      "Iteration 253, loss = 0.09867463\n",
      "Iteration 254, loss = 0.09823700\n",
      "Iteration 255, loss = 0.09787578\n",
      "Iteration 256, loss = 0.09748618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.09707600\n",
      "Iteration 258, loss = 0.09704526\n",
      "Iteration 259, loss = 0.09653287\n",
      "Iteration 260, loss = 0.09595599\n",
      "Iteration 261, loss = 0.09538141\n",
      "Iteration 262, loss = 0.09510779\n",
      "Iteration 263, loss = 0.09490533\n",
      "Iteration 264, loss = 0.09436323\n",
      "Iteration 265, loss = 0.09395226\n",
      "Iteration 266, loss = 0.09366528\n",
      "Iteration 267, loss = 0.09312664\n",
      "Iteration 268, loss = 0.09298778\n",
      "Iteration 269, loss = 0.09258787\n",
      "Iteration 270, loss = 0.09202324\n",
      "Iteration 271, loss = 0.09179022\n",
      "Iteration 272, loss = 0.09146980\n",
      "Iteration 273, loss = 0.09110464\n",
      "Iteration 274, loss = 0.09068603\n",
      "Iteration 275, loss = 0.09054709\n",
      "Iteration 276, loss = 0.08987251\n",
      "Iteration 277, loss = 0.08962690\n",
      "Iteration 278, loss = 0.08934997\n",
      "Iteration 279, loss = 0.08893108\n",
      "Iteration 280, loss = 0.08876492\n",
      "Iteration 281, loss = 0.08824363\n",
      "Iteration 282, loss = 0.08804358\n",
      "Iteration 283, loss = 0.08763006\n",
      "Iteration 284, loss = 0.08766867\n",
      "Iteration 285, loss = 0.08707063\n",
      "Iteration 286, loss = 0.08669633\n",
      "Iteration 287, loss = 0.08640837\n",
      "Iteration 288, loss = 0.08604645\n",
      "Iteration 289, loss = 0.08575260\n",
      "Iteration 290, loss = 0.08533214\n",
      "Iteration 291, loss = 0.08518577\n",
      "Iteration 292, loss = 0.08472548\n",
      "Iteration 293, loss = 0.08446912\n",
      "Iteration 294, loss = 0.08423039\n",
      "Iteration 295, loss = 0.08388292\n",
      "Iteration 296, loss = 0.08344810\n",
      "Iteration 297, loss = 0.08316706\n",
      "Iteration 298, loss = 0.08280357\n",
      "Iteration 299, loss = 0.08249845\n",
      "Iteration 300, loss = 0.08232350\n",
      "Iteration 301, loss = 0.08200891\n",
      "Iteration 302, loss = 0.08159607\n",
      "Iteration 303, loss = 0.08140005\n",
      "Iteration 304, loss = 0.08135051\n",
      "Iteration 305, loss = 0.08087221\n",
      "Iteration 306, loss = 0.08058892\n",
      "Iteration 307, loss = 0.08016493\n",
      "Iteration 308, loss = 0.07999259\n",
      "Iteration 309, loss = 0.07968453\n",
      "Iteration 310, loss = 0.07940439\n",
      "Iteration 311, loss = 0.07905822\n",
      "Iteration 312, loss = 0.07893718\n",
      "Iteration 313, loss = 0.07851438\n",
      "Iteration 314, loss = 0.07821368\n",
      "Iteration 315, loss = 0.07795644\n",
      "Iteration 316, loss = 0.07764276\n",
      "Iteration 317, loss = 0.07752986\n",
      "Iteration 318, loss = 0.07715321\n",
      "Iteration 319, loss = 0.07709104\n",
      "Iteration 320, loss = 0.07659561\n",
      "Iteration 321, loss = 0.07634499\n",
      "Iteration 322, loss = 0.07607683\n",
      "Iteration 323, loss = 0.07578570\n",
      "Iteration 324, loss = 0.07561434\n",
      "Iteration 325, loss = 0.07539223\n",
      "Iteration 326, loss = 0.07502938\n",
      "Iteration 327, loss = 0.07487195\n",
      "Iteration 328, loss = 0.07445033\n",
      "Iteration 329, loss = 0.07419484\n",
      "Iteration 330, loss = 0.07411534\n",
      "Iteration 331, loss = 0.07390241\n",
      "Iteration 332, loss = 0.07345672\n",
      "Iteration 333, loss = 0.07346064\n",
      "Iteration 334, loss = 0.07302793\n",
      "Iteration 335, loss = 0.07277603\n",
      "Iteration 336, loss = 0.07296197\n",
      "Iteration 337, loss = 0.07252455\n",
      "Iteration 338, loss = 0.07206154\n",
      "Iteration 339, loss = 0.07192390\n",
      "Iteration 340, loss = 0.07165089\n",
      "Iteration 341, loss = 0.07134990\n",
      "Iteration 342, loss = 0.07115220\n",
      "Iteration 343, loss = 0.07083286\n",
      "Iteration 344, loss = 0.07060187\n",
      "Iteration 345, loss = 0.07039393\n",
      "Iteration 346, loss = 0.07026519\n",
      "Iteration 347, loss = 0.06996925\n",
      "Iteration 348, loss = 0.06978949\n",
      "Iteration 349, loss = 0.06940778\n",
      "Iteration 350, loss = 0.06928782\n",
      "Iteration 351, loss = 0.06900193\n",
      "Iteration 352, loss = 0.06877521\n",
      "Iteration 353, loss = 0.06863169\n",
      "Iteration 354, loss = 0.06835096\n",
      "Iteration 355, loss = 0.06813627\n",
      "Iteration 356, loss = 0.06812862\n",
      "Iteration 357, loss = 0.06768695\n",
      "Iteration 358, loss = 0.06745043\n",
      "Iteration 359, loss = 0.06727158\n",
      "Iteration 360, loss = 0.06718180\n",
      "Iteration 361, loss = 0.06699644\n",
      "Iteration 362, loss = 0.06672867\n",
      "Iteration 363, loss = 0.06645363\n",
      "Iteration 364, loss = 0.06670021\n",
      "Iteration 365, loss = 0.06604232\n",
      "Iteration 366, loss = 0.06585811\n",
      "Iteration 367, loss = 0.06557468\n",
      "Iteration 368, loss = 0.06538543\n",
      "Iteration 369, loss = 0.06557013\n",
      "Iteration 370, loss = 0.06499098\n",
      "Iteration 371, loss = 0.06487486\n",
      "Iteration 372, loss = 0.06455560\n",
      "Iteration 373, loss = 0.06445631\n",
      "Iteration 374, loss = 0.06416828\n",
      "Iteration 375, loss = 0.06392157\n",
      "Iteration 376, loss = 0.06379086\n",
      "Iteration 377, loss = 0.06370172\n",
      "Iteration 378, loss = 0.06354053\n",
      "Iteration 379, loss = 0.06322787\n",
      "Iteration 380, loss = 0.06322927\n",
      "Iteration 381, loss = 0.06312016\n",
      "Iteration 382, loss = 0.06263292\n",
      "Iteration 383, loss = 0.06238462\n",
      "Iteration 384, loss = 0.06224827\n",
      "Iteration 385, loss = 0.06201598\n",
      "Iteration 386, loss = 0.06184793\n",
      "Iteration 387, loss = 0.06160639\n",
      "Iteration 388, loss = 0.06141900\n",
      "Iteration 389, loss = 0.06119676\n",
      "Iteration 390, loss = 0.06111250\n",
      "Iteration 391, loss = 0.06086227\n",
      "Iteration 392, loss = 0.06077803\n",
      "Iteration 393, loss = 0.06050284\n",
      "Iteration 394, loss = 0.06041861\n",
      "Iteration 395, loss = 0.06018294\n",
      "Iteration 396, loss = 0.05995992\n",
      "Iteration 397, loss = 0.05976568\n",
      "Iteration 398, loss = 0.05962670\n",
      "Iteration 399, loss = 0.05938902\n",
      "Iteration 400, loss = 0.05932527\n",
      "Iteration 401, loss = 0.05919932\n",
      "Iteration 402, loss = 0.05899500\n",
      "Iteration 403, loss = 0.05898449\n",
      "Iteration 404, loss = 0.05874949\n",
      "Iteration 405, loss = 0.05848502\n",
      "Iteration 406, loss = 0.05828537\n",
      "Iteration 407, loss = 0.05805700\n",
      "Iteration 408, loss = 0.05792507\n",
      "Iteration 409, loss = 0.05774345\n",
      "Iteration 410, loss = 0.05766516\n",
      "Iteration 411, loss = 0.05742848\n",
      "Iteration 412, loss = 0.05729792\n",
      "Iteration 413, loss = 0.05724178\n",
      "Iteration 414, loss = 0.05696653\n",
      "Iteration 415, loss = 0.05676705\n",
      "Iteration 416, loss = 0.05658438\n",
      "Iteration 417, loss = 0.05648062\n",
      "Iteration 418, loss = 0.05631433\n",
      "Iteration 419, loss = 0.05608107\n",
      "Iteration 420, loss = 0.05599513\n",
      "Iteration 421, loss = 0.05581304\n",
      "Iteration 422, loss = 0.05572401\n",
      "Iteration 423, loss = 0.05563460\n",
      "Iteration 424, loss = 0.05545477\n",
      "Iteration 425, loss = 0.05520396\n",
      "Iteration 426, loss = 0.05500145\n",
      "Iteration 427, loss = 0.05492372\n",
      "Iteration 428, loss = 0.05479870\n",
      "Iteration 429, loss = 0.05460197\n",
      "Iteration 430, loss = 0.05449434\n",
      "Iteration 431, loss = 0.05432921\n",
      "Iteration 432, loss = 0.05411164\n",
      "Iteration 433, loss = 0.05403243\n",
      "Iteration 434, loss = 0.05385661\n",
      "Iteration 435, loss = 0.05366690\n",
      "Iteration 436, loss = 0.05354602\n",
      "Iteration 437, loss = 0.05339324\n",
      "Iteration 438, loss = 0.05337672\n",
      "Iteration 439, loss = 0.05329113\n",
      "Iteration 440, loss = 0.05294926\n",
      "Iteration 441, loss = 0.05282074\n",
      "Iteration 442, loss = 0.05267808\n",
      "Iteration 443, loss = 0.05257407\n",
      "Iteration 444, loss = 0.05248554\n",
      "Iteration 445, loss = 0.05224117\n",
      "Iteration 446, loss = 0.05214758\n",
      "Iteration 447, loss = 0.05199617\n",
      "Iteration 448, loss = 0.05188649\n",
      "Iteration 449, loss = 0.05180415\n",
      "Iteration 450, loss = 0.05176652\n",
      "Iteration 451, loss = 0.05152662\n",
      "Iteration 452, loss = 0.05137001\n",
      "Iteration 453, loss = 0.05128794\n",
      "Iteration 454, loss = 0.05104990\n",
      "Iteration 455, loss = 0.05084837\n",
      "Iteration 456, loss = 0.05072755\n",
      "Iteration 457, loss = 0.05074013\n",
      "Iteration 458, loss = 0.05066887\n",
      "Iteration 459, loss = 0.05036610\n",
      "Iteration 460, loss = 0.05030416\n",
      "Iteration 461, loss = 0.05016386\n",
      "Iteration 462, loss = 0.05005318\n",
      "Iteration 463, loss = 0.04996448\n",
      "Iteration 464, loss = 0.04983663\n",
      "Iteration 465, loss = 0.04983593\n",
      "Iteration 466, loss = 0.04945120\n",
      "Iteration 467, loss = 0.04952503\n",
      "Iteration 468, loss = 0.04920252\n",
      "Iteration 469, loss = 0.04911470\n",
      "Iteration 470, loss = 0.04894851\n",
      "Iteration 471, loss = 0.04883555\n",
      "Iteration 472, loss = 0.04877159\n",
      "Iteration 473, loss = 0.04854469\n",
      "Iteration 474, loss = 0.04843657\n",
      "Iteration 475, loss = 0.04835788\n",
      "Iteration 476, loss = 0.04823561\n",
      "Iteration 477, loss = 0.04826113\n",
      "Iteration 478, loss = 0.04795057\n",
      "Iteration 479, loss = 0.04803782\n",
      "Iteration 480, loss = 0.04794439\n",
      "Iteration 481, loss = 0.04771459\n",
      "Iteration 482, loss = 0.04771163\n",
      "Iteration 483, loss = 0.04758387\n",
      "Iteration 484, loss = 0.04738383\n",
      "Iteration 485, loss = 0.04731046\n",
      "Iteration 486, loss = 0.04713876\n",
      "Iteration 487, loss = 0.04716933\n",
      "Iteration 488, loss = 0.04677541\n",
      "Iteration 489, loss = 0.04670261\n",
      "Iteration 490, loss = 0.04663053\n",
      "Iteration 491, loss = 0.04645699\n",
      "Iteration 492, loss = 0.04629622\n",
      "Iteration 493, loss = 0.04628108\n",
      "Iteration 494, loss = 0.04611094\n",
      "Iteration 495, loss = 0.04601375\n",
      "Iteration 496, loss = 0.04590218\n",
      "Iteration 497, loss = 0.04592749\n",
      "Iteration 498, loss = 0.04563875\n",
      "Iteration 499, loss = 0.04550317\n",
      "Iteration 500, loss = 0.04541499\n",
      "Iteration 501, loss = 0.04539639\n",
      "Iteration 502, loss = 0.04518890\n",
      "Iteration 503, loss = 0.04518242\n",
      "Iteration 504, loss = 0.04498785\n",
      "Iteration 505, loss = 0.04492549\n",
      "Iteration 506, loss = 0.04481750\n",
      "Iteration 507, loss = 0.04467091\n",
      "Iteration 508, loss = 0.04454693\n",
      "Iteration 509, loss = 0.04445186\n",
      "Iteration 510, loss = 0.04435700\n",
      "Iteration 511, loss = 0.04423366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 512, loss = 0.04443190\n",
      "Iteration 513, loss = 0.04405169\n",
      "Iteration 514, loss = 0.04391656\n",
      "Iteration 515, loss = 0.04387935\n",
      "Iteration 516, loss = 0.04385222\n",
      "Iteration 517, loss = 0.04368874\n",
      "Iteration 518, loss = 0.04351239\n",
      "Iteration 519, loss = 0.04340938\n",
      "Iteration 520, loss = 0.04333451\n",
      "Iteration 521, loss = 0.04316312\n",
      "Iteration 522, loss = 0.04309707\n",
      "Iteration 523, loss = 0.04296701\n",
      "Iteration 524, loss = 0.04296160\n",
      "Iteration 525, loss = 0.04279572\n",
      "Iteration 526, loss = 0.04267870\n",
      "Iteration 527, loss = 0.04264282\n",
      "Iteration 528, loss = 0.04247208\n",
      "Iteration 529, loss = 0.04248462\n",
      "Iteration 530, loss = 0.04228893\n",
      "Iteration 531, loss = 0.04234511\n",
      "Iteration 532, loss = 0.04208436\n",
      "Iteration 533, loss = 0.04202700\n",
      "Iteration 534, loss = 0.04189425\n",
      "Iteration 535, loss = 0.04191841\n",
      "Iteration 536, loss = 0.04174524\n",
      "Iteration 537, loss = 0.04164498\n",
      "Iteration 538, loss = 0.04154056\n",
      "Iteration 539, loss = 0.04144480\n",
      "Iteration 540, loss = 0.04135402\n",
      "Iteration 541, loss = 0.04124147\n",
      "Iteration 542, loss = 0.04120079\n",
      "Iteration 543, loss = 0.04112547\n",
      "Iteration 544, loss = 0.04096376\n",
      "Iteration 545, loss = 0.04093460\n",
      "Iteration 546, loss = 0.04080335\n",
      "Iteration 547, loss = 0.04093707\n",
      "Iteration 548, loss = 0.04060915\n",
      "Iteration 549, loss = 0.04047813\n",
      "Iteration 550, loss = 0.04042259\n",
      "Iteration 551, loss = 0.04039637\n",
      "Iteration 552, loss = 0.04026139\n",
      "Iteration 553, loss = 0.04019607\n",
      "Iteration 554, loss = 0.04005468\n",
      "Iteration 555, loss = 0.04004329\n",
      "Iteration 556, loss = 0.03986298\n",
      "Iteration 557, loss = 0.03986822\n",
      "Iteration 558, loss = 0.03971832\n",
      "Iteration 559, loss = 0.03960791\n",
      "Iteration 560, loss = 0.03950977\n",
      "Iteration 561, loss = 0.03945625\n",
      "Iteration 562, loss = 0.03934508\n",
      "Iteration 563, loss = 0.03924923\n",
      "Iteration 564, loss = 0.03920049\n",
      "Iteration 565, loss = 0.03911640\n",
      "Iteration 566, loss = 0.03901573\n",
      "Iteration 567, loss = 0.03896124\n",
      "Iteration 568, loss = 0.03898265\n",
      "Iteration 569, loss = 0.03902075\n",
      "Iteration 570, loss = 0.03863412\n",
      "Iteration 571, loss = 0.03870930\n",
      "Iteration 572, loss = 0.03878576\n",
      "Iteration 573, loss = 0.03853023\n",
      "Iteration 574, loss = 0.03846696\n",
      "Iteration 575, loss = 0.03831294\n",
      "Iteration 576, loss = 0.03823484\n",
      "Iteration 577, loss = 0.03819716\n",
      "Iteration 578, loss = 0.03806183\n",
      "Iteration 579, loss = 0.03791942\n",
      "Iteration 580, loss = 0.03786286\n",
      "Iteration 581, loss = 0.03792052\n",
      "Iteration 582, loss = 0.03765169\n",
      "Iteration 583, loss = 0.03770933\n",
      "Iteration 584, loss = 0.03754569\n",
      "Iteration 585, loss = 0.03752340\n",
      "Iteration 586, loss = 0.03734323\n",
      "Iteration 587, loss = 0.03729272\n",
      "Iteration 588, loss = 0.03721431\n",
      "Iteration 589, loss = 0.03712677\n",
      "Iteration 590, loss = 0.03708792\n",
      "Iteration 591, loss = 0.03701608\n",
      "Iteration 592, loss = 0.03690121\n",
      "Iteration 593, loss = 0.03691734\n",
      "Iteration 594, loss = 0.03673499\n",
      "Iteration 595, loss = 0.03674778\n",
      "Iteration 596, loss = 0.03660244\n",
      "Iteration 597, loss = 0.03649602\n",
      "Iteration 598, loss = 0.03645907\n",
      "Iteration 599, loss = 0.03653934\n",
      "Iteration 600, loss = 0.03629395\n",
      "Iteration 601, loss = 0.03632779\n",
      "Iteration 602, loss = 0.03621926\n",
      "Iteration 603, loss = 0.03609317\n",
      "Iteration 604, loss = 0.03601415\n",
      "Iteration 605, loss = 0.03589979\n",
      "Iteration 606, loss = 0.03588786\n",
      "Iteration 607, loss = 0.03573723\n",
      "Iteration 608, loss = 0.03569142\n",
      "Iteration 609, loss = 0.03562002\n",
      "Iteration 610, loss = 0.03557761\n",
      "Iteration 611, loss = 0.03546256\n",
      "Iteration 612, loss = 0.03541931\n",
      "Iteration 613, loss = 0.03551292\n",
      "Iteration 614, loss = 0.03526460\n",
      "Iteration 615, loss = 0.03517834\n",
      "Iteration 616, loss = 0.03514483\n",
      "Iteration 617, loss = 0.03506614\n",
      "Iteration 618, loss = 0.03496999\n",
      "Iteration 619, loss = 0.03489911\n",
      "Iteration 620, loss = 0.03482442\n",
      "Iteration 621, loss = 0.03484130\n",
      "Iteration 622, loss = 0.03469397\n",
      "Iteration 623, loss = 0.03465398\n",
      "Iteration 624, loss = 0.03452665\n",
      "Iteration 625, loss = 0.03453331\n",
      "Iteration 626, loss = 0.03440843\n",
      "Iteration 627, loss = 0.03454216\n",
      "Iteration 628, loss = 0.03423504\n",
      "Iteration 629, loss = 0.03439076\n",
      "Iteration 630, loss = 0.03414334\n",
      "Iteration 631, loss = 0.03419205\n",
      "Iteration 632, loss = 0.03401548\n",
      "Iteration 633, loss = 0.03392789\n",
      "Iteration 634, loss = 0.03385223\n",
      "Iteration 635, loss = 0.03394358\n",
      "Iteration 636, loss = 0.03385462\n",
      "Iteration 637, loss = 0.03369673\n",
      "Iteration 638, loss = 0.03358499\n",
      "Iteration 639, loss = 0.03353694\n",
      "Iteration 640, loss = 0.03344222\n",
      "Iteration 641, loss = 0.03339369\n",
      "Iteration 642, loss = 0.03331632\n",
      "Iteration 643, loss = 0.03343759\n",
      "Iteration 644, loss = 0.03320604\n",
      "Iteration 645, loss = 0.03313898\n",
      "Iteration 646, loss = 0.03307591\n",
      "Iteration 647, loss = 0.03297784\n",
      "Iteration 648, loss = 0.03299290\n",
      "Iteration 649, loss = 0.03292939\n",
      "Iteration 650, loss = 0.03293972\n",
      "Iteration 651, loss = 0.03275670\n",
      "Iteration 652, loss = 0.03267005\n",
      "Iteration 653, loss = 0.03275573\n",
      "Iteration 654, loss = 0.03265844\n",
      "Iteration 655, loss = 0.03261600\n",
      "Iteration 656, loss = 0.03252348\n",
      "Iteration 657, loss = 0.03255678\n",
      "Iteration 658, loss = 0.03239758\n",
      "Iteration 659, loss = 0.03232366\n",
      "Iteration 660, loss = 0.03219976\n",
      "Iteration 661, loss = 0.03211899\n",
      "Iteration 662, loss = 0.03207345\n",
      "Iteration 663, loss = 0.03204013\n",
      "Iteration 664, loss = 0.03205253\n",
      "Iteration 665, loss = 0.03199824\n",
      "Iteration 666, loss = 0.03182368\n",
      "Iteration 667, loss = 0.03178291\n",
      "Iteration 668, loss = 0.03174436\n",
      "Iteration 669, loss = 0.03164754\n",
      "Iteration 670, loss = 0.03162170\n",
      "Iteration 671, loss = 0.03154382\n",
      "Iteration 672, loss = 0.03150143\n",
      "Iteration 673, loss = 0.03148264\n",
      "Iteration 674, loss = 0.03148028\n",
      "Iteration 675, loss = 0.03135144\n",
      "Iteration 676, loss = 0.03126600\n",
      "Iteration 677, loss = 0.03129502\n",
      "Iteration 678, loss = 0.03112395\n",
      "Iteration 679, loss = 0.03103908\n",
      "Iteration 680, loss = 0.03098942\n",
      "Iteration 681, loss = 0.03109272\n",
      "Iteration 682, loss = 0.03086586\n",
      "Iteration 683, loss = 0.03085793\n",
      "Iteration 684, loss = 0.03075535\n",
      "Iteration 685, loss = 0.03079596\n",
      "Iteration 686, loss = 0.03062490\n",
      "Iteration 687, loss = 0.03057887\n",
      "Iteration 688, loss = 0.03056626\n",
      "Iteration 689, loss = 0.03053374\n",
      "Iteration 690, loss = 0.03045911\n",
      "Iteration 691, loss = 0.03034914\n",
      "Iteration 692, loss = 0.03038135\n",
      "Iteration 693, loss = 0.03033680\n",
      "Iteration 694, loss = 0.03020633\n",
      "Iteration 695, loss = 0.03013259\n",
      "Iteration 696, loss = 0.03007488\n",
      "Iteration 697, loss = 0.03005424\n",
      "Iteration 698, loss = 0.03024177\n",
      "Iteration 699, loss = 0.02992812\n",
      "Iteration 700, loss = 0.02989599\n",
      "Iteration 701, loss = 0.02984063\n",
      "Iteration 702, loss = 0.02979879\n",
      "Iteration 703, loss = 0.02971686\n",
      "Iteration 704, loss = 0.02968034\n",
      "Iteration 705, loss = 0.02963236\n",
      "Iteration 706, loss = 0.02955479\n",
      "Iteration 707, loss = 0.02952372\n",
      "Iteration 708, loss = 0.02948095\n",
      "Iteration 709, loss = 0.02941551\n",
      "Iteration 710, loss = 0.02937586\n",
      "Iteration 711, loss = 0.02927546\n",
      "Iteration 712, loss = 0.02928055\n",
      "Iteration 713, loss = 0.02918831\n",
      "Iteration 714, loss = 0.02922248\n",
      "Iteration 715, loss = 0.02911383\n",
      "Iteration 716, loss = 0.02903189\n",
      "Iteration 717, loss = 0.02910008\n",
      "Iteration 718, loss = 0.02900343\n",
      "Iteration 719, loss = 0.02889407\n",
      "Iteration 720, loss = 0.02885115\n",
      "Iteration 721, loss = 0.02878449\n",
      "Iteration 722, loss = 0.02876559\n",
      "Iteration 723, loss = 0.02871062\n",
      "Iteration 724, loss = 0.02866216\n",
      "Iteration 725, loss = 0.02856260\n",
      "Iteration 726, loss = 0.02853452\n",
      "Iteration 727, loss = 0.02856708\n",
      "Iteration 728, loss = 0.02844514\n",
      "Iteration 729, loss = 0.02846823\n",
      "Iteration 730, loss = 0.02834784\n",
      "Iteration 731, loss = 0.02830313\n",
      "Iteration 732, loss = 0.02824193\n",
      "Iteration 733, loss = 0.02822392\n",
      "Iteration 734, loss = 0.02819253\n",
      "Iteration 735, loss = 0.02810406\n",
      "Iteration 736, loss = 0.02802141\n",
      "Iteration 737, loss = 0.02803166\n",
      "Iteration 738, loss = 0.02798847\n",
      "Iteration 739, loss = 0.02785763\n",
      "Iteration 740, loss = 0.02782741\n",
      "Iteration 741, loss = 0.02780692\n",
      "Iteration 742, loss = 0.02773053\n",
      "Iteration 743, loss = 0.02768038\n",
      "Iteration 744, loss = 0.02772668\n",
      "Iteration 745, loss = 0.02760977\n",
      "Iteration 746, loss = 0.02754237\n",
      "Iteration 747, loss = 0.02754617\n",
      "Iteration 748, loss = 0.02749990\n",
      "Iteration 749, loss = 0.02745209\n",
      "Iteration 750, loss = 0.02740007\n",
      "Iteration 751, loss = 0.02732151\n",
      "Iteration 752, loss = 0.02731382\n",
      "Iteration 753, loss = 0.02722132\n",
      "Iteration 754, loss = 0.02716892\n",
      "Iteration 755, loss = 0.02710213\n",
      "Iteration 756, loss = 0.02707663\n",
      "Iteration 757, loss = 0.02703885\n",
      "Iteration 758, loss = 0.02702267\n",
      "Iteration 759, loss = 0.02695029\n",
      "Iteration 760, loss = 0.02691929\n",
      "Iteration 761, loss = 0.02690681\n",
      "Iteration 762, loss = 0.02682564\n",
      "Iteration 763, loss = 0.02675853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 764, loss = 0.02670998\n",
      "Iteration 765, loss = 0.02668977\n",
      "Iteration 766, loss = 0.02660936\n",
      "Iteration 767, loss = 0.02657236\n",
      "Iteration 768, loss = 0.02654951\n",
      "Iteration 769, loss = 0.02651161\n",
      "Iteration 770, loss = 0.02643154\n",
      "Iteration 771, loss = 0.02638772\n",
      "Iteration 772, loss = 0.02636049\n",
      "Iteration 773, loss = 0.02631896\n",
      "Iteration 774, loss = 0.02624439\n",
      "Iteration 775, loss = 0.02627705\n",
      "Iteration 776, loss = 0.02618525\n",
      "Iteration 777, loss = 0.02614980\n",
      "Iteration 778, loss = 0.02610545\n",
      "Iteration 779, loss = 0.02604695\n",
      "Iteration 780, loss = 0.02606707\n",
      "Iteration 781, loss = 0.02603544\n",
      "Iteration 782, loss = 0.02592273\n",
      "Iteration 783, loss = 0.02587354\n",
      "Iteration 784, loss = 0.02582100\n",
      "Iteration 785, loss = 0.02581277\n",
      "Iteration 786, loss = 0.02578877\n",
      "Iteration 787, loss = 0.02571010\n",
      "Iteration 788, loss = 0.02569580\n",
      "Iteration 789, loss = 0.02570273\n",
      "Iteration 790, loss = 0.02561743\n",
      "Iteration 791, loss = 0.02557262\n",
      "Iteration 792, loss = 0.02550785\n",
      "Iteration 793, loss = 0.02550616\n",
      "Iteration 794, loss = 0.02543606\n",
      "Iteration 795, loss = 0.02539765\n",
      "Iteration 796, loss = 0.02539135\n",
      "Iteration 797, loss = 0.02533391\n",
      "Iteration 798, loss = 0.02527085\n",
      "Iteration 799, loss = 0.02523500\n",
      "Iteration 800, loss = 0.02519788\n",
      "Iteration 801, loss = 0.02514317\n",
      "Iteration 802, loss = 0.02510674\n",
      "Iteration 803, loss = 0.02509386\n",
      "Iteration 804, loss = 0.02505453\n",
      "Iteration 805, loss = 0.02505679\n",
      "Iteration 806, loss = 0.02495252\n",
      "Iteration 807, loss = 0.02490906\n",
      "Iteration 808, loss = 0.02484217\n",
      "Iteration 809, loss = 0.02481409\n",
      "Iteration 810, loss = 0.02480643\n",
      "Iteration 811, loss = 0.02473627\n",
      "Iteration 812, loss = 0.02474075\n",
      "Iteration 813, loss = 0.02470637\n",
      "Iteration 814, loss = 0.02465066\n",
      "Iteration 815, loss = 0.02458844\n",
      "Iteration 816, loss = 0.02455563\n",
      "Iteration 817, loss = 0.02453967\n",
      "Iteration 818, loss = 0.02447304\n",
      "Iteration 819, loss = 0.02442560\n",
      "Iteration 820, loss = 0.02441702\n",
      "Iteration 821, loss = 0.02436340\n",
      "Iteration 822, loss = 0.02432792\n",
      "Iteration 823, loss = 0.02429029\n",
      "Iteration 824, loss = 0.02428845\n",
      "Iteration 825, loss = 0.02418912\n",
      "Iteration 826, loss = 0.02415529\n",
      "Iteration 827, loss = 0.02414130\n",
      "Iteration 828, loss = 0.02415909\n",
      "Iteration 829, loss = 0.02403970\n",
      "Iteration 830, loss = 0.02409921\n",
      "Iteration 831, loss = 0.02404759\n",
      "Iteration 832, loss = 0.02396853\n",
      "Iteration 833, loss = 0.02390715\n",
      "Iteration 834, loss = 0.02388874\n",
      "Iteration 835, loss = 0.02382241\n",
      "Iteration 836, loss = 0.02378745\n",
      "Iteration 837, loss = 0.02377770\n",
      "Iteration 838, loss = 0.02376173\n",
      "Iteration 839, loss = 0.02368549\n",
      "Iteration 840, loss = 0.02362832\n",
      "Iteration 841, loss = 0.02359768\n",
      "Iteration 842, loss = 0.02364560\n",
      "Iteration 843, loss = 0.02357172\n",
      "Iteration 844, loss = 0.02350220\n",
      "Iteration 845, loss = 0.02350475\n",
      "Iteration 846, loss = 0.02347625\n",
      "Iteration 847, loss = 0.02338149\n",
      "Iteration 848, loss = 0.02335748\n",
      "Iteration 849, loss = 0.02334284\n",
      "Iteration 850, loss = 0.02327813\n",
      "Iteration 851, loss = 0.02328786\n",
      "Iteration 852, loss = 0.02326581\n",
      "Iteration 853, loss = 0.02317951\n",
      "Iteration 854, loss = 0.02319103\n",
      "Iteration 855, loss = 0.02311749\n",
      "Iteration 856, loss = 0.02319895\n",
      "Iteration 857, loss = 0.02301598\n",
      "Iteration 858, loss = 0.02301917\n",
      "Iteration 859, loss = 0.02304978\n",
      "Iteration 860, loss = 0.02298206\n",
      "Iteration 861, loss = 0.02298372\n",
      "Iteration 862, loss = 0.02286793\n",
      "Iteration 863, loss = 0.02287781\n",
      "Iteration 864, loss = 0.02278866\n",
      "Iteration 865, loss = 0.02278663\n",
      "Iteration 866, loss = 0.02274403\n",
      "Iteration 867, loss = 0.02269759\n",
      "Iteration 868, loss = 0.02264865\n",
      "Iteration 869, loss = 0.02272251\n",
      "Iteration 870, loss = 0.02264279\n",
      "Iteration 871, loss = 0.02253379\n",
      "Iteration 872, loss = 0.02256524\n",
      "Iteration 873, loss = 0.02252918\n",
      "Iteration 874, loss = 0.02246593\n",
      "Iteration 875, loss = 0.02243492\n",
      "Iteration 876, loss = 0.02249031\n",
      "Iteration 877, loss = 0.02236496\n",
      "Iteration 878, loss = 0.02234422\n",
      "Iteration 879, loss = 0.02228442\n",
      "Iteration 880, loss = 0.02226868\n",
      "Iteration 881, loss = 0.02222101\n",
      "Iteration 882, loss = 0.02225331\n",
      "Iteration 883, loss = 0.02217298\n",
      "Iteration 884, loss = 0.02213859\n",
      "Iteration 885, loss = 0.02209919\n",
      "Iteration 886, loss = 0.02206761\n",
      "Iteration 887, loss = 0.02202318\n",
      "Iteration 888, loss = 0.02203657\n",
      "Iteration 889, loss = 0.02197957\n",
      "Iteration 890, loss = 0.02197558\n",
      "Iteration 891, loss = 0.02191552\n",
      "Iteration 892, loss = 0.02190672\n",
      "Iteration 893, loss = 0.02186469\n",
      "Iteration 894, loss = 0.02185557\n",
      "Iteration 895, loss = 0.02179346\n",
      "Iteration 896, loss = 0.02177965\n",
      "Iteration 897, loss = 0.02171286\n",
      "Iteration 898, loss = 0.02177008\n",
      "Iteration 899, loss = 0.02165777\n",
      "Iteration 900, loss = 0.02165713\n",
      "Iteration 901, loss = 0.02161067\n",
      "Iteration 902, loss = 0.02158400\n",
      "Iteration 903, loss = 0.02156259\n",
      "Iteration 904, loss = 0.02153691\n",
      "Iteration 905, loss = 0.02146580\n",
      "Iteration 906, loss = 0.02143231\n",
      "Iteration 907, loss = 0.02142306\n",
      "Iteration 908, loss = 0.02138884\n",
      "Iteration 909, loss = 0.02149299\n",
      "Iteration 910, loss = 0.02134692\n",
      "Iteration 911, loss = 0.02130263\n",
      "Iteration 912, loss = 0.02127775\n",
      "Iteration 913, loss = 0.02122641\n",
      "Iteration 914, loss = 0.02126117\n",
      "Iteration 915, loss = 0.02120804\n",
      "Iteration 916, loss = 0.02111843\n",
      "Iteration 917, loss = 0.02115490\n",
      "Iteration 918, loss = 0.02111689\n",
      "Iteration 919, loss = 0.02104719\n",
      "Iteration 920, loss = 0.02100318\n",
      "Iteration 921, loss = 0.02096824\n",
      "Iteration 922, loss = 0.02097245\n",
      "Iteration 923, loss = 0.02091556\n",
      "Iteration 924, loss = 0.02089273\n",
      "Iteration 925, loss = 0.02086632\n",
      "Iteration 926, loss = 0.02089477\n",
      "Iteration 927, loss = 0.02081966\n",
      "Iteration 928, loss = 0.02079699\n",
      "Iteration 929, loss = 0.02074121\n",
      "Iteration 930, loss = 0.02073399\n",
      "Iteration 931, loss = 0.02073875\n",
      "Iteration 932, loss = 0.02071004\n",
      "Iteration 933, loss = 0.02069824\n",
      "Iteration 934, loss = 0.02061717\n",
      "Iteration 935, loss = 0.02056943\n",
      "Iteration 936, loss = 0.02054650\n",
      "Iteration 937, loss = 0.02052213\n",
      "Iteration 938, loss = 0.02052880\n",
      "Iteration 939, loss = 0.02048762\n",
      "Iteration 940, loss = 0.02044060\n",
      "Iteration 941, loss = 0.02040737\n",
      "Iteration 942, loss = 0.02038291\n",
      "Iteration 943, loss = 0.02036651\n",
      "Iteration 944, loss = 0.02035999\n",
      "Iteration 945, loss = 0.02029209\n",
      "Iteration 946, loss = 0.02031133\n",
      "Iteration 947, loss = 0.02027362\n",
      "Iteration 948, loss = 0.02023538\n",
      "Iteration 949, loss = 0.02023499\n",
      "Iteration 950, loss = 0.02022271\n",
      "Iteration 951, loss = 0.02012810\n",
      "Iteration 952, loss = 0.02008775\n",
      "Iteration 953, loss = 0.02007080\n",
      "Iteration 954, loss = 0.02004343\n",
      "Iteration 955, loss = 0.02002801\n",
      "Iteration 956, loss = 0.02000785\n",
      "Iteration 957, loss = 0.01994895\n",
      "Iteration 958, loss = 0.01998291\n",
      "Iteration 959, loss = 0.01994814\n",
      "Iteration 960, loss = 0.01994474\n",
      "Iteration 961, loss = 0.01992299\n",
      "Iteration 962, loss = 0.01992356\n",
      "Iteration 963, loss = 0.01981203\n",
      "Iteration 964, loss = 0.01976812\n",
      "Iteration 965, loss = 0.01979162\n",
      "Iteration 966, loss = 0.01973510\n",
      "Iteration 967, loss = 0.01974653\n",
      "Iteration 968, loss = 0.01965979\n",
      "Iteration 969, loss = 0.01963437\n",
      "Iteration 970, loss = 0.01967997\n",
      "Iteration 971, loss = 0.01961813\n",
      "Iteration 972, loss = 0.01955095\n",
      "Iteration 973, loss = 0.01953963\n",
      "Iteration 974, loss = 0.01952574\n",
      "Iteration 975, loss = 0.01947726\n",
      "Iteration 976, loss = 0.01945593\n",
      "Iteration 977, loss = 0.01946346\n",
      "Iteration 978, loss = 0.01941820\n",
      "Iteration 979, loss = 0.01938556\n",
      "Iteration 980, loss = 0.01934790\n",
      "Iteration 981, loss = 0.01932902\n",
      "Iteration 982, loss = 0.01932817\n",
      "Iteration 983, loss = 0.01929053\n",
      "Iteration 984, loss = 0.01927997\n",
      "Iteration 985, loss = 0.01923329\n",
      "Iteration 986, loss = 0.01918666\n",
      "Iteration 987, loss = 0.01921908\n",
      "Iteration 988, loss = 0.01913686\n",
      "Iteration 989, loss = 0.01914798\n",
      "Iteration 990, loss = 0.01910425\n",
      "Iteration 991, loss = 0.01913209\n",
      "Iteration 992, loss = 0.01904514\n",
      "Iteration 993, loss = 0.01902695\n",
      "Iteration 994, loss = 0.01904577\n",
      "Iteration 995, loss = 0.01897746\n",
      "Iteration 996, loss = 0.01893841\n",
      "Iteration 997, loss = 0.01893131\n",
      "Iteration 998, loss = 0.01892499\n",
      "Iteration 999, loss = 0.01889938\n",
      "Iteration 1000, loss = 0.01886633\n"
     ]
    }
   ],
   "source": [
    "nnModelSGD= mlpSGD.fit(trainData, trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(90,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n",
       "              tol=1e-19, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnModelSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70335182\n",
      "Iteration 2, loss = 1.12383767\n",
      "Iteration 3, loss = 0.87320107\n",
      "Iteration 4, loss = 0.71283727\n",
      "Iteration 5, loss = 0.59945291\n",
      "Iteration 6, loss = 0.50578379\n",
      "Iteration 7, loss = 0.44285982\n",
      "Iteration 8, loss = 0.38378832\n",
      "Iteration 9, loss = 0.34056101\n",
      "Iteration 10, loss = 0.30842474\n",
      "Iteration 11, loss = 0.29034673\n",
      "Iteration 12, loss = 0.25554764\n",
      "Iteration 13, loss = 0.23049465\n",
      "Iteration 14, loss = 0.21028710\n",
      "Iteration 15, loss = 0.19563122\n",
      "Iteration 16, loss = 0.18315378\n",
      "Iteration 17, loss = 0.16724746\n",
      "Iteration 18, loss = 0.15470793\n",
      "Iteration 19, loss = 0.14494894\n",
      "Iteration 20, loss = 0.13622106\n",
      "Iteration 21, loss = 0.12652499\n",
      "Iteration 22, loss = 0.12073758\n",
      "Iteration 23, loss = 0.11523384\n",
      "Iteration 24, loss = 0.10925567\n",
      "Iteration 25, loss = 0.10017128\n",
      "Iteration 26, loss = 0.09581806\n",
      "Iteration 27, loss = 0.09402882\n",
      "Iteration 28, loss = 0.08448306\n",
      "Iteration 29, loss = 0.08133278\n",
      "Iteration 30, loss = 0.08368344\n",
      "Iteration 31, loss = 0.07545815\n",
      "Iteration 32, loss = 0.07221295\n",
      "Iteration 33, loss = 0.06791432\n",
      "Iteration 34, loss = 0.06510795\n",
      "Iteration 35, loss = 0.06236526\n",
      "Iteration 36, loss = 0.06207550\n",
      "Iteration 37, loss = 0.05823355\n",
      "Iteration 38, loss = 0.05642961\n",
      "Iteration 39, loss = 0.05336648\n",
      "Iteration 40, loss = 0.05047205\n",
      "Iteration 41, loss = 0.04796697\n",
      "Iteration 42, loss = 0.04663611\n",
      "Iteration 43, loss = 0.04519371\n",
      "Iteration 44, loss = 0.04323200\n",
      "Iteration 45, loss = 0.04203526\n",
      "Iteration 46, loss = 0.04082406\n",
      "Iteration 47, loss = 0.03909649\n",
      "Iteration 48, loss = 0.03728394\n",
      "Iteration 49, loss = 0.03600461\n",
      "Iteration 50, loss = 0.03567852\n",
      "Iteration 51, loss = 0.03518630\n",
      "Iteration 52, loss = 0.03407639\n",
      "Iteration 53, loss = 0.03222161\n",
      "Iteration 54, loss = 0.03124851\n",
      "Iteration 55, loss = 0.02984617\n",
      "Iteration 56, loss = 0.02930420\n",
      "Iteration 57, loss = 0.02792536\n",
      "Iteration 58, loss = 0.02747342\n",
      "Iteration 59, loss = 0.02725936\n",
      "Iteration 60, loss = 0.02749528\n",
      "Iteration 61, loss = 0.02489918\n",
      "Iteration 62, loss = 0.02405008\n",
      "Iteration 63, loss = 0.02384719\n",
      "Iteration 64, loss = 0.02295016\n",
      "Iteration 65, loss = 0.02240289\n",
      "Iteration 66, loss = 0.02214227\n",
      "Iteration 67, loss = 0.02114799\n",
      "Iteration 68, loss = 0.02100036\n",
      "Iteration 69, loss = 0.02019763\n",
      "Iteration 70, loss = 0.01952150\n",
      "Iteration 71, loss = 0.02062570\n",
      "Iteration 72, loss = 0.01993243\n",
      "Iteration 73, loss = 0.01879910\n",
      "Iteration 74, loss = 0.01819258\n",
      "Iteration 75, loss = 0.01763872\n",
      "Iteration 76, loss = 0.01664094\n",
      "Iteration 77, loss = 0.01634564\n",
      "Iteration 78, loss = 0.01556294\n",
      "Iteration 79, loss = 0.01505870\n",
      "Iteration 80, loss = 0.01503440\n",
      "Iteration 81, loss = 0.01495538\n",
      "Iteration 82, loss = 0.01414414\n",
      "Iteration 83, loss = 0.01362279\n",
      "Iteration 84, loss = 0.01333821\n",
      "Iteration 85, loss = 0.01306515\n",
      "Iteration 86, loss = 0.01266922\n",
      "Iteration 87, loss = 0.01232650\n",
      "Iteration 88, loss = 0.01200597\n",
      "Iteration 89, loss = 0.01175281\n",
      "Iteration 90, loss = 0.01158946\n",
      "Iteration 91, loss = 0.01128983\n",
      "Iteration 92, loss = 0.01107136\n",
      "Iteration 93, loss = 0.01070509\n",
      "Iteration 94, loss = 0.01036970\n",
      "Iteration 95, loss = 0.01020288\n",
      "Iteration 96, loss = 0.01034639\n",
      "Iteration 97, loss = 0.00984651\n",
      "Iteration 98, loss = 0.00995469\n",
      "Iteration 99, loss = 0.00959225\n",
      "Iteration 100, loss = 0.00951864\n",
      "Iteration 101, loss = 0.00914954\n",
      "Iteration 102, loss = 0.00872011\n",
      "Iteration 103, loss = 0.00849620\n",
      "Iteration 104, loss = 0.00830066\n",
      "Iteration 105, loss = 0.00808495\n",
      "Iteration 106, loss = 0.00800069\n",
      "Iteration 107, loss = 0.00790766\n",
      "Iteration 108, loss = 0.00765863\n",
      "Iteration 109, loss = 0.00762259\n",
      "Iteration 110, loss = 0.00738318\n",
      "Iteration 111, loss = 0.00715113\n",
      "Iteration 112, loss = 0.00703904\n",
      "Iteration 113, loss = 0.00684415\n",
      "Iteration 114, loss = 0.00672846\n",
      "Iteration 115, loss = 0.00656247\n",
      "Iteration 116, loss = 0.00655200\n",
      "Iteration 117, loss = 0.00636018\n",
      "Iteration 118, loss = 0.00622749\n",
      "Iteration 119, loss = 0.00616772\n",
      "Iteration 120, loss = 0.00603542\n",
      "Iteration 121, loss = 0.00617872\n",
      "Iteration 122, loss = 0.00611897\n",
      "Iteration 123, loss = 0.00566807\n",
      "Iteration 124, loss = 0.00546913\n",
      "Iteration 125, loss = 0.00541199\n",
      "Iteration 126, loss = 0.00541490\n",
      "Iteration 127, loss = 0.00529814\n",
      "Iteration 128, loss = 0.00511508\n",
      "Iteration 129, loss = 0.00502361\n",
      "Iteration 130, loss = 0.00496404\n",
      "Iteration 131, loss = 0.00482706\n",
      "Iteration 132, loss = 0.00482417\n",
      "Iteration 133, loss = 0.00465638\n",
      "Iteration 134, loss = 0.00458114\n",
      "Iteration 135, loss = 0.00448425\n",
      "Iteration 136, loss = 0.00442473\n",
      "Iteration 137, loss = 0.00436296\n",
      "Iteration 138, loss = 0.00433717\n",
      "Iteration 139, loss = 0.00421274\n",
      "Iteration 140, loss = 0.00418107\n",
      "Iteration 141, loss = 0.00411216\n",
      "Iteration 142, loss = 0.00418279\n",
      "Iteration 143, loss = 0.00418323\n",
      "Iteration 144, loss = 0.00400155\n",
      "Iteration 145, loss = 0.00377743\n",
      "Iteration 146, loss = 0.00378460\n",
      "Iteration 147, loss = 0.00367303\n",
      "Iteration 148, loss = 0.00357489\n",
      "Iteration 149, loss = 0.00351480\n",
      "Iteration 150, loss = 0.00352938\n",
      "Iteration 151, loss = 0.00357585\n",
      "Iteration 152, loss = 0.00345224\n",
      "Iteration 153, loss = 0.00337214\n",
      "Iteration 154, loss = 0.00327018\n",
      "Iteration 155, loss = 0.00321231\n",
      "Iteration 156, loss = 0.00315339\n",
      "Iteration 157, loss = 0.00307684\n",
      "Iteration 158, loss = 0.00309266\n",
      "Iteration 159, loss = 0.00303738\n",
      "Iteration 160, loss = 0.00295384\n",
      "Iteration 161, loss = 0.00303935\n",
      "Iteration 162, loss = 0.00291834\n",
      "Iteration 163, loss = 0.00283225\n",
      "Iteration 164, loss = 0.00279059\n",
      "Iteration 165, loss = 0.00279765\n",
      "Iteration 166, loss = 0.00276309\n",
      "Iteration 167, loss = 0.00273068\n",
      "Iteration 168, loss = 0.00266569\n",
      "Iteration 169, loss = 0.00275112\n",
      "Iteration 170, loss = 0.00259336\n",
      "Iteration 171, loss = 0.00265128\n",
      "Iteration 172, loss = 0.00251910\n",
      "Iteration 173, loss = 0.00246847\n",
      "Iteration 174, loss = 0.00245470\n",
      "Iteration 175, loss = 0.00238281\n",
      "Iteration 176, loss = 0.00237783\n",
      "Iteration 177, loss = 0.00230424\n",
      "Iteration 178, loss = 0.00228650\n",
      "Iteration 179, loss = 0.00227795\n",
      "Iteration 180, loss = 0.00229086\n",
      "Iteration 181, loss = 0.00219300\n",
      "Iteration 182, loss = 0.00218963\n",
      "Iteration 183, loss = 0.00214165\n",
      "Iteration 184, loss = 0.00210989\n",
      "Iteration 185, loss = 0.00208418\n",
      "Iteration 186, loss = 0.00205684\n",
      "Iteration 187, loss = 0.00203346\n",
      "Iteration 188, loss = 0.00200092\n",
      "Iteration 189, loss = 0.00199375\n",
      "Iteration 190, loss = 0.00195115\n",
      "Iteration 191, loss = 0.00192247\n",
      "Iteration 192, loss = 0.00190063\n",
      "Iteration 193, loss = 0.00187746\n",
      "Iteration 194, loss = 0.00187375\n",
      "Iteration 195, loss = 0.00184426\n",
      "Iteration 196, loss = 0.00181511\n",
      "Iteration 197, loss = 0.00179469\n",
      "Iteration 198, loss = 0.00177278\n",
      "Iteration 199, loss = 0.00173572\n",
      "Iteration 200, loss = 0.00173370\n",
      "Iteration 201, loss = 0.00169840\n",
      "Iteration 202, loss = 0.00169533\n",
      "Iteration 203, loss = 0.00168021\n",
      "Iteration 204, loss = 0.00163488\n",
      "Iteration 205, loss = 0.00163871\n",
      "Iteration 206, loss = 0.00161710\n",
      "Iteration 207, loss = 0.00158699\n",
      "Iteration 208, loss = 0.00157508\n",
      "Iteration 209, loss = 0.00155363\n",
      "Iteration 210, loss = 0.00153138\n",
      "Iteration 211, loss = 0.00154070\n",
      "Iteration 212, loss = 0.00152097\n",
      "Iteration 213, loss = 0.00149039\n",
      "Iteration 214, loss = 0.00148206\n",
      "Iteration 215, loss = 0.00145980\n",
      "Iteration 216, loss = 0.00145055\n",
      "Iteration 217, loss = 0.00148244\n",
      "Iteration 218, loss = 0.00149931\n",
      "Iteration 219, loss = 0.00136158\n",
      "Iteration 220, loss = 0.00141796\n",
      "Iteration 221, loss = 0.00136307\n",
      "Iteration 222, loss = 0.00139061\n",
      "Iteration 223, loss = 0.00147864\n",
      "Iteration 224, loss = 0.00129513\n",
      "Iteration 225, loss = 0.00139182\n",
      "Iteration 226, loss = 0.00132390\n",
      "Iteration 227, loss = 0.00127451\n",
      "Iteration 228, loss = 0.00127619\n",
      "Iteration 229, loss = 0.00125465\n",
      "Iteration 230, loss = 0.00124336\n",
      "Iteration 231, loss = 0.00125231\n",
      "Iteration 232, loss = 0.00122067\n",
      "Iteration 233, loss = 0.00120921\n",
      "Iteration 234, loss = 0.00119376\n",
      "Iteration 235, loss = 0.00118510\n",
      "Iteration 236, loss = 0.00117217\n",
      "Iteration 237, loss = 0.00116139\n",
      "Iteration 238, loss = 0.00116304\n",
      "Iteration 239, loss = 0.00114753\n",
      "Iteration 240, loss = 0.00112598\n",
      "Iteration 241, loss = 0.00111955\n",
      "Iteration 242, loss = 0.00112330\n",
      "Iteration 243, loss = 0.00111449\n",
      "Iteration 244, loss = 0.00110056\n",
      "Iteration 245, loss = 0.00107527\n",
      "Iteration 246, loss = 0.00107437\n",
      "Iteration 247, loss = 0.00105868\n",
      "Iteration 248, loss = 0.00104834\n",
      "Iteration 249, loss = 0.00104489\n",
      "Iteration 250, loss = 0.00104689\n",
      "Iteration 251, loss = 0.00102657\n",
      "Iteration 252, loss = 0.00102615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.00101876\n",
      "Iteration 254, loss = 0.00099049\n",
      "Iteration 255, loss = 0.00098189\n",
      "Iteration 256, loss = 0.00098182\n",
      "Iteration 257, loss = 0.00096809\n",
      "Iteration 258, loss = 0.00097725\n",
      "Iteration 259, loss = 0.00098230\n",
      "Iteration 260, loss = 0.00095092\n",
      "Iteration 261, loss = 0.00094566\n",
      "Iteration 262, loss = 0.00093599\n",
      "Iteration 263, loss = 0.00093368\n",
      "Iteration 264, loss = 0.00091649\n",
      "Iteration 265, loss = 0.00090657\n",
      "Iteration 266, loss = 0.00089669\n",
      "Iteration 267, loss = 0.00089017\n",
      "Iteration 268, loss = 0.00088954\n",
      "Iteration 269, loss = 0.00086955\n",
      "Iteration 270, loss = 0.00086786\n",
      "Iteration 271, loss = 0.00086803\n",
      "Iteration 272, loss = 0.00085392\n",
      "Iteration 273, loss = 0.00084986\n",
      "Iteration 274, loss = 0.00084694\n",
      "Iteration 275, loss = 0.00085186\n",
      "Iteration 276, loss = 0.00082686\n",
      "Iteration 277, loss = 0.00082389\n",
      "Iteration 278, loss = 0.00081408\n",
      "Iteration 279, loss = 0.00080516\n",
      "Iteration 280, loss = 0.00080135\n",
      "Iteration 281, loss = 0.00079101\n",
      "Iteration 282, loss = 0.00079269\n",
      "Iteration 283, loss = 0.00077928\n",
      "Iteration 284, loss = 0.00077701\n",
      "Iteration 285, loss = 0.00077643\n",
      "Iteration 286, loss = 0.00077136\n",
      "Iteration 287, loss = 0.00076147\n",
      "Iteration 288, loss = 0.00075057\n",
      "Iteration 289, loss = 0.00074972\n",
      "Iteration 290, loss = 0.00073811\n",
      "Iteration 291, loss = 0.00073716\n",
      "Iteration 292, loss = 0.00072860\n",
      "Iteration 293, loss = 0.00073139\n",
      "Iteration 294, loss = 0.00072077\n",
      "Iteration 295, loss = 0.00071659\n",
      "Iteration 296, loss = 0.00070649\n",
      "Iteration 297, loss = 0.00070351\n",
      "Iteration 298, loss = 0.00069564\n",
      "Iteration 299, loss = 0.00069340\n",
      "Iteration 300, loss = 0.00068980\n",
      "Iteration 301, loss = 0.00068197\n",
      "Iteration 302, loss = 0.00067745\n",
      "Iteration 303, loss = 0.00067927\n",
      "Iteration 304, loss = 0.00067975\n",
      "Iteration 305, loss = 0.00066162\n",
      "Iteration 306, loss = 0.00066429\n",
      "Iteration 307, loss = 0.00065349\n",
      "Iteration 308, loss = 0.00065594\n",
      "Iteration 309, loss = 0.00064481\n",
      "Iteration 310, loss = 0.00064437\n",
      "Iteration 311, loss = 0.00063721\n",
      "Iteration 312, loss = 0.00063858\n",
      "Iteration 313, loss = 0.00062928\n",
      "Iteration 314, loss = 0.00062279\n",
      "Iteration 315, loss = 0.00062028\n",
      "Iteration 316, loss = 0.00061665\n",
      "Iteration 317, loss = 0.00061257\n",
      "Iteration 318, loss = 0.00061520\n",
      "Iteration 319, loss = 0.00060644\n",
      "Iteration 320, loss = 0.00059970\n",
      "Iteration 321, loss = 0.00059842\n",
      "Iteration 322, loss = 0.00059273\n",
      "Iteration 323, loss = 0.00059135\n",
      "Iteration 324, loss = 0.00059172\n",
      "Iteration 325, loss = 0.00058216\n",
      "Iteration 326, loss = 0.00057675\n",
      "Iteration 327, loss = 0.00057590\n",
      "Iteration 328, loss = 0.00056770\n",
      "Iteration 329, loss = 0.00056731\n",
      "Iteration 330, loss = 0.00056365\n",
      "Iteration 331, loss = 0.00056177\n",
      "Iteration 332, loss = 0.00055723\n",
      "Iteration 333, loss = 0.00055138\n",
      "Iteration 334, loss = 0.00054745\n",
      "Iteration 335, loss = 0.00054608\n",
      "Iteration 336, loss = 0.00055063\n",
      "Iteration 337, loss = 0.00054562\n",
      "Iteration 338, loss = 0.00053680\n",
      "Iteration 339, loss = 0.00054708\n",
      "Iteration 340, loss = 0.00053446\n",
      "Iteration 341, loss = 0.00053357\n",
      "Iteration 342, loss = 0.00052234\n",
      "Iteration 343, loss = 0.00051893\n",
      "Iteration 344, loss = 0.00051476\n",
      "Iteration 345, loss = 0.00051164\n",
      "Iteration 346, loss = 0.00050945\n",
      "Iteration 347, loss = 0.00050743\n",
      "Iteration 348, loss = 0.00050639\n",
      "Iteration 349, loss = 0.00050159\n",
      "Iteration 350, loss = 0.00049935\n",
      "Iteration 351, loss = 0.00049681\n",
      "Iteration 352, loss = 0.00049304\n",
      "Iteration 353, loss = 0.00048973\n",
      "Iteration 354, loss = 0.00048572\n",
      "Iteration 355, loss = 0.00048235\n",
      "Iteration 356, loss = 0.00048070\n",
      "Iteration 357, loss = 0.00047831\n",
      "Iteration 358, loss = 0.00047510\n",
      "Iteration 359, loss = 0.00047228\n",
      "Iteration 360, loss = 0.00047362\n",
      "Iteration 361, loss = 0.00046960\n",
      "Iteration 362, loss = 0.00046753\n",
      "Iteration 363, loss = 0.00046507\n",
      "Iteration 364, loss = 0.00046261\n",
      "Iteration 365, loss = 0.00045634\n",
      "Iteration 366, loss = 0.00045489\n",
      "Iteration 367, loss = 0.00045659\n",
      "Iteration 368, loss = 0.00044939\n",
      "Iteration 369, loss = 0.00044903\n",
      "Iteration 370, loss = 0.00044437\n",
      "Iteration 371, loss = 0.00044557\n",
      "Iteration 372, loss = 0.00043997\n",
      "Iteration 373, loss = 0.00043794\n",
      "Iteration 374, loss = 0.00043460\n",
      "Iteration 375, loss = 0.00043332\n",
      "Iteration 376, loss = 0.00043165\n",
      "Iteration 377, loss = 0.00042909\n",
      "Iteration 378, loss = 0.00042615\n",
      "Iteration 379, loss = 0.00042567\n",
      "Iteration 380, loss = 0.00042021\n",
      "Iteration 381, loss = 0.00041955\n",
      "Iteration 382, loss = 0.00041792\n",
      "Iteration 383, loss = 0.00041570\n",
      "Iteration 384, loss = 0.00041268\n",
      "Iteration 385, loss = 0.00041025\n",
      "Iteration 386, loss = 0.00041033\n",
      "Iteration 387, loss = 0.00040585\n",
      "Iteration 388, loss = 0.00040439\n",
      "Iteration 389, loss = 0.00040200\n",
      "Iteration 390, loss = 0.00040057\n",
      "Iteration 391, loss = 0.00039869\n",
      "Iteration 392, loss = 0.00039594\n",
      "Iteration 393, loss = 0.00039597\n",
      "Iteration 394, loss = 0.00039333\n",
      "Iteration 395, loss = 0.00039093\n",
      "Iteration 396, loss = 0.00038817\n",
      "Iteration 397, loss = 0.00038649\n",
      "Iteration 398, loss = 0.00038435\n",
      "Iteration 399, loss = 0.00038256\n",
      "Iteration 400, loss = 0.00038067\n",
      "Iteration 401, loss = 0.00038342\n",
      "Iteration 402, loss = 0.00037644\n",
      "Iteration 403, loss = 0.00038326\n",
      "Iteration 404, loss = 0.00038202\n",
      "Iteration 405, loss = 0.00037480\n",
      "Iteration 406, loss = 0.00037288\n",
      "Iteration 407, loss = 0.00037017\n",
      "Iteration 408, loss = 0.00036694\n",
      "Iteration 409, loss = 0.00036534\n",
      "Iteration 410, loss = 0.00037115\n",
      "Iteration 411, loss = 0.00036338\n",
      "Iteration 412, loss = 0.00036308\n",
      "Iteration 413, loss = 0.00035716\n",
      "Iteration 414, loss = 0.00035732\n",
      "Iteration 415, loss = 0.00035877\n",
      "Iteration 416, loss = 0.00035338\n",
      "Iteration 417, loss = 0.00035233\n",
      "Iteration 418, loss = 0.00035087\n",
      "Iteration 419, loss = 0.00034781\n",
      "Iteration 420, loss = 0.00034684\n",
      "Iteration 421, loss = 0.00034520\n",
      "Iteration 422, loss = 0.00034280\n",
      "Iteration 423, loss = 0.00034208\n",
      "Iteration 424, loss = 0.00034264\n",
      "Iteration 425, loss = 0.00033845\n",
      "Iteration 426, loss = 0.00033669\n",
      "Iteration 427, loss = 0.00033874\n",
      "Iteration 428, loss = 0.00033523\n",
      "Iteration 429, loss = 0.00033336\n",
      "Iteration 430, loss = 0.00033122\n",
      "Iteration 431, loss = 0.00033156\n",
      "Iteration 432, loss = 0.00033004\n",
      "Iteration 433, loss = 0.00032759\n",
      "Iteration 434, loss = 0.00032511\n",
      "Iteration 435, loss = 0.00032538\n",
      "Iteration 436, loss = 0.00032216\n",
      "Iteration 437, loss = 0.00032194\n",
      "Iteration 438, loss = 0.00031960\n",
      "Iteration 439, loss = 0.00031964\n",
      "Iteration 440, loss = 0.00031691\n",
      "Iteration 441, loss = 0.00031575\n",
      "Iteration 442, loss = 0.00031433\n",
      "Iteration 443, loss = 0.00031348\n",
      "Iteration 444, loss = 0.00031551\n",
      "Iteration 445, loss = 0.00031092\n",
      "Iteration 446, loss = 0.00031095\n",
      "Iteration 447, loss = 0.00031027\n",
      "Iteration 448, loss = 0.00030723\n",
      "Iteration 449, loss = 0.00030614\n",
      "Iteration 450, loss = 0.00030527\n",
      "Iteration 451, loss = 0.00030348\n",
      "Iteration 452, loss = 0.00030192\n",
      "Iteration 453, loss = 0.00030080\n",
      "Iteration 454, loss = 0.00029928\n",
      "Iteration 455, loss = 0.00029782\n",
      "Iteration 456, loss = 0.00029655\n",
      "Iteration 457, loss = 0.00029555\n",
      "Iteration 458, loss = 0.00029473\n",
      "Iteration 459, loss = 0.00029325\n",
      "Iteration 460, loss = 0.00029238\n",
      "Iteration 461, loss = 0.00029075\n",
      "Iteration 462, loss = 0.00029100\n",
      "Iteration 463, loss = 0.00028860\n",
      "Iteration 464, loss = 0.00028812\n",
      "Iteration 465, loss = 0.00028760\n",
      "Iteration 466, loss = 0.00028631\n",
      "Iteration 467, loss = 0.00028428\n",
      "Iteration 468, loss = 0.00028282\n",
      "Iteration 469, loss = 0.00028157\n",
      "Iteration 470, loss = 0.00028134\n",
      "Iteration 471, loss = 0.00027943\n",
      "Iteration 472, loss = 0.00027880\n",
      "Iteration 473, loss = 0.00027742\n",
      "Iteration 474, loss = 0.00027618\n",
      "Iteration 475, loss = 0.00027543\n",
      "Iteration 476, loss = 0.00027408\n",
      "Iteration 477, loss = 0.00027718\n",
      "Iteration 478, loss = 0.00027220\n",
      "Iteration 479, loss = 0.00027273\n",
      "Iteration 480, loss = 0.00027063\n",
      "Iteration 481, loss = 0.00026931\n",
      "Iteration 482, loss = 0.00026838\n",
      "Iteration 483, loss = 0.00026800\n",
      "Iteration 484, loss = 0.00026849\n",
      "Iteration 485, loss = 0.00026587\n",
      "Iteration 486, loss = 0.00026523\n",
      "Iteration 487, loss = 0.00026763\n",
      "Iteration 488, loss = 0.00026241\n",
      "Iteration 489, loss = 0.00026405\n",
      "Iteration 490, loss = 0.00026177\n",
      "Iteration 491, loss = 0.00026061\n",
      "Iteration 492, loss = 0.00025915\n",
      "Iteration 493, loss = 0.00025863\n",
      "Iteration 494, loss = 0.00025809\n",
      "Iteration 495, loss = 0.00025787\n",
      "Iteration 496, loss = 0.00025602\n",
      "Iteration 497, loss = 0.00025615\n",
      "Iteration 498, loss = 0.00025472\n",
      "Iteration 499, loss = 0.00025388\n",
      "Iteration 500, loss = 0.00025157\n",
      "Iteration 501, loss = 0.00025269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 502, loss = 0.00024886\n",
      "Iteration 503, loss = 0.00024898\n",
      "Iteration 504, loss = 0.00024698\n",
      "Iteration 505, loss = 0.00024645\n",
      "Iteration 506, loss = 0.00024559\n",
      "Iteration 507, loss = 0.00024644\n",
      "Iteration 508, loss = 0.00024279\n",
      "Iteration 509, loss = 0.00024300\n",
      "Iteration 510, loss = 0.00024089\n",
      "Iteration 511, loss = 0.00023966\n",
      "Iteration 512, loss = 0.00023854\n",
      "Iteration 513, loss = 0.00023753\n",
      "Iteration 514, loss = 0.00023599\n",
      "Iteration 515, loss = 0.00023467\n",
      "Iteration 516, loss = 0.00023620\n",
      "Iteration 517, loss = 0.00023312\n",
      "Iteration 518, loss = 0.00023263\n",
      "Iteration 519, loss = 0.00022914\n",
      "Iteration 520, loss = 0.00022699\n",
      "Iteration 521, loss = 0.00022628\n",
      "Iteration 522, loss = 0.00022464\n",
      "Iteration 523, loss = 0.00022220\n",
      "Iteration 524, loss = 0.00022051\n",
      "Iteration 525, loss = 0.00021953\n",
      "Iteration 526, loss = 0.00021685\n",
      "Iteration 527, loss = 0.00021601\n",
      "Iteration 528, loss = 0.00021377\n",
      "Iteration 529, loss = 0.00021236\n",
      "Iteration 530, loss = 0.00021022\n",
      "Iteration 531, loss = 0.00021130\n",
      "Iteration 532, loss = 0.00020651\n",
      "Iteration 533, loss = 0.00020499\n",
      "Iteration 534, loss = 0.00020438\n",
      "Iteration 535, loss = 0.00020148\n",
      "Iteration 536, loss = 0.00020025\n",
      "Iteration 537, loss = 0.00019871\n",
      "Iteration 538, loss = 0.00019681\n",
      "Iteration 539, loss = 0.00019580\n",
      "Iteration 540, loss = 0.00019597\n",
      "Iteration 541, loss = 0.00019261\n",
      "Iteration 542, loss = 0.00019150\n",
      "Iteration 543, loss = 0.00019012\n",
      "Iteration 544, loss = 0.00018845\n",
      "Iteration 545, loss = 0.00019038\n",
      "Iteration 546, loss = 0.00018682\n",
      "Iteration 547, loss = 0.00018308\n",
      "Iteration 548, loss = 0.00018387\n",
      "Iteration 549, loss = 0.00018242\n",
      "Iteration 550, loss = 0.00018174\n",
      "Iteration 551, loss = 0.00018296\n",
      "Iteration 552, loss = 0.00018026\n",
      "Iteration 553, loss = 0.00017682\n",
      "Iteration 554, loss = 0.00017560\n",
      "Iteration 555, loss = 0.00017794\n",
      "Iteration 556, loss = 0.00017424\n",
      "Iteration 557, loss = 0.00017380\n",
      "Iteration 558, loss = 0.00017101\n",
      "Iteration 559, loss = 0.00017026\n",
      "Iteration 560, loss = 0.00016872\n",
      "Iteration 561, loss = 0.00016801\n",
      "Iteration 562, loss = 0.00016660\n",
      "Iteration 563, loss = 0.00016683\n",
      "Iteration 564, loss = 0.00016645\n",
      "Iteration 565, loss = 0.00016544\n",
      "Iteration 566, loss = 0.00016307\n",
      "Iteration 567, loss = 0.00016264\n",
      "Iteration 568, loss = 0.00016137\n",
      "Iteration 569, loss = 0.00016083\n",
      "Iteration 570, loss = 0.00015988\n",
      "Iteration 571, loss = 0.00016020\n",
      "Iteration 572, loss = 0.00015683\n",
      "Iteration 573, loss = 0.00015938\n",
      "Iteration 574, loss = 0.00015667\n",
      "Iteration 575, loss = 0.00015798\n",
      "Iteration 576, loss = 0.00015903\n",
      "Iteration 577, loss = 0.00015713\n",
      "Iteration 578, loss = 0.00015624\n",
      "Iteration 579, loss = 0.00015677\n",
      "Iteration 580, loss = 0.00015284\n",
      "Iteration 581, loss = 0.00015168\n",
      "Iteration 582, loss = 0.00015093\n",
      "Iteration 583, loss = 0.00014966\n",
      "Iteration 584, loss = 0.00015034\n",
      "Iteration 585, loss = 0.00015231\n",
      "Iteration 586, loss = 0.00014759\n",
      "Iteration 587, loss = 0.00014868\n",
      "Iteration 588, loss = 0.00014792\n",
      "Iteration 589, loss = 0.00014621\n",
      "Iteration 590, loss = 0.00014570\n",
      "Iteration 591, loss = 0.00014543\n",
      "Iteration 592, loss = 0.00014457\n",
      "Iteration 593, loss = 0.00014401\n",
      "Iteration 594, loss = 0.00014315\n",
      "Iteration 595, loss = 0.00014260\n",
      "Iteration 596, loss = 0.00014162\n",
      "Iteration 597, loss = 0.00014161\n",
      "Iteration 598, loss = 0.00014064\n",
      "Iteration 599, loss = 0.00014100\n",
      "Iteration 600, loss = 0.00013964\n",
      "Iteration 601, loss = 0.00014001\n",
      "Iteration 602, loss = 0.00014143\n",
      "Iteration 603, loss = 0.00014017\n",
      "Iteration 604, loss = 0.00013808\n",
      "Iteration 605, loss = 0.00013850\n",
      "Iteration 606, loss = 0.00013756\n",
      "Iteration 607, loss = 0.00013668\n",
      "Iteration 608, loss = 0.00013723\n",
      "Iteration 609, loss = 0.00013536\n",
      "Iteration 610, loss = 0.00013551\n",
      "Iteration 611, loss = 0.00013445\n",
      "Iteration 612, loss = 0.00013416\n",
      "Iteration 613, loss = 0.00013521\n",
      "Iteration 614, loss = 0.00013406\n",
      "Iteration 615, loss = 0.00013321\n",
      "Iteration 616, loss = 0.00013257\n",
      "Iteration 617, loss = 0.00013246\n",
      "Iteration 618, loss = 0.00013146\n",
      "Iteration 619, loss = 0.00013142\n",
      "Iteration 620, loss = 0.00013086\n",
      "Iteration 621, loss = 0.00013064\n",
      "Iteration 622, loss = 0.00013143\n",
      "Iteration 623, loss = 0.00012955\n",
      "Iteration 624, loss = 0.00012958\n",
      "Iteration 625, loss = 0.00012934\n",
      "Iteration 626, loss = 0.00012859\n",
      "Iteration 627, loss = 0.00012845\n",
      "Iteration 628, loss = 0.00012811\n",
      "Iteration 629, loss = 0.00012738\n",
      "Iteration 630, loss = 0.00012742\n",
      "Iteration 631, loss = 0.00012686\n",
      "Iteration 632, loss = 0.00012672\n",
      "Iteration 633, loss = 0.00012629\n",
      "Iteration 634, loss = 0.00012581\n",
      "Iteration 635, loss = 0.00012616\n",
      "Iteration 636, loss = 0.00012557\n",
      "Iteration 637, loss = 0.00012551\n",
      "Iteration 638, loss = 0.00012465\n",
      "Iteration 639, loss = 0.00012424\n",
      "Iteration 640, loss = 0.00012389\n",
      "Iteration 641, loss = 0.00012367\n",
      "Iteration 642, loss = 0.00012345\n",
      "Iteration 643, loss = 0.00012340\n",
      "Iteration 644, loss = 0.00012328\n",
      "Iteration 645, loss = 0.00012242\n",
      "Iteration 646, loss = 0.00012223\n",
      "Iteration 647, loss = 0.00012184\n",
      "Iteration 648, loss = 0.00012174\n",
      "Iteration 649, loss = 0.00012154\n",
      "Iteration 650, loss = 0.00012108\n",
      "Iteration 651, loss = 0.00012079\n",
      "Iteration 652, loss = 0.00012053\n",
      "Iteration 653, loss = 0.00012037\n",
      "Iteration 654, loss = 0.00012036\n",
      "Iteration 655, loss = 0.00011982\n",
      "Iteration 656, loss = 0.00011962\n",
      "Iteration 657, loss = 0.00011968\n",
      "Iteration 658, loss = 0.00011969\n",
      "Iteration 659, loss = 0.00011954\n",
      "Iteration 660, loss = 0.00011836\n",
      "Iteration 661, loss = 0.00011896\n",
      "Iteration 662, loss = 0.00011802\n",
      "Iteration 663, loss = 0.00011809\n",
      "Iteration 664, loss = 0.00011840\n",
      "Iteration 665, loss = 0.00011770\n",
      "Iteration 666, loss = 0.00011727\n",
      "Iteration 667, loss = 0.00011712\n",
      "Iteration 668, loss = 0.00011669\n",
      "Iteration 669, loss = 0.00011639\n",
      "Iteration 670, loss = 0.00011615\n",
      "Iteration 671, loss = 0.00011584\n",
      "Iteration 672, loss = 0.00011589\n",
      "Iteration 673, loss = 0.00011564\n",
      "Iteration 674, loss = 0.00011556\n",
      "Iteration 675, loss = 0.00011513\n",
      "Iteration 676, loss = 0.00011492\n",
      "Iteration 677, loss = 0.00011485\n",
      "Iteration 678, loss = 0.00011442\n",
      "Iteration 679, loss = 0.00011411\n",
      "Iteration 680, loss = 0.00011407\n",
      "Iteration 681, loss = 0.00011383\n",
      "Iteration 682, loss = 0.00011353\n",
      "Iteration 683, loss = 0.00011348\n",
      "Iteration 684, loss = 0.00011342\n",
      "Iteration 685, loss = 0.00011295\n",
      "Iteration 686, loss = 0.00011293\n",
      "Iteration 687, loss = 0.00011272\n",
      "Iteration 688, loss = 0.00011258\n",
      "Iteration 689, loss = 0.00011237\n",
      "Iteration 690, loss = 0.00011215\n",
      "Iteration 691, loss = 0.00011211\n",
      "Iteration 692, loss = 0.00011171\n",
      "Iteration 693, loss = 0.00011171\n",
      "Iteration 694, loss = 0.00011143\n",
      "Iteration 695, loss = 0.00011125\n",
      "Iteration 696, loss = 0.00011096\n",
      "Iteration 697, loss = 0.00011079\n",
      "Iteration 698, loss = 0.00011099\n",
      "Iteration 699, loss = 0.00011065\n",
      "Iteration 700, loss = 0.00011038\n",
      "Iteration 701, loss = 0.00011092\n",
      "Iteration 702, loss = 0.00011064\n",
      "Iteration 703, loss = 0.00011029\n",
      "Iteration 704, loss = 0.00010979\n",
      "Iteration 705, loss = 0.00010990\n",
      "Iteration 706, loss = 0.00010949\n",
      "Iteration 707, loss = 0.00010943\n",
      "Iteration 708, loss = 0.00010924\n",
      "Iteration 709, loss = 0.00010924\n",
      "Iteration 710, loss = 0.00010909\n",
      "Iteration 711, loss = 0.00010849\n",
      "Iteration 712, loss = 0.00010862\n",
      "Iteration 713, loss = 0.00010836\n",
      "Iteration 714, loss = 0.00010810\n",
      "Iteration 715, loss = 0.00010803\n",
      "Iteration 716, loss = 0.00010783\n",
      "Iteration 717, loss = 0.00010784\n",
      "Iteration 718, loss = 0.00010759\n",
      "Iteration 719, loss = 0.00010739\n",
      "Iteration 720, loss = 0.00010737\n",
      "Iteration 721, loss = 0.00010717\n",
      "Iteration 722, loss = 0.00010699\n",
      "Iteration 723, loss = 0.00010689\n",
      "Iteration 724, loss = 0.00010671\n",
      "Iteration 725, loss = 0.00010660\n",
      "Iteration 726, loss = 0.00010658\n",
      "Iteration 727, loss = 0.00010635\n",
      "Iteration 728, loss = 0.00010622\n",
      "Iteration 729, loss = 0.00010606\n",
      "Iteration 730, loss = 0.00010603\n",
      "Iteration 731, loss = 0.00010592\n",
      "Iteration 732, loss = 0.00010570\n",
      "Iteration 733, loss = 0.00010563\n",
      "Iteration 734, loss = 0.00010546\n",
      "Iteration 735, loss = 0.00010531\n",
      "Iteration 736, loss = 0.00010541\n",
      "Iteration 737, loss = 0.00010499\n",
      "Iteration 738, loss = 0.00010508\n",
      "Iteration 739, loss = 0.00010492\n",
      "Iteration 740, loss = 0.00010462\n",
      "Iteration 741, loss = 0.00010455\n",
      "Iteration 742, loss = 0.00010445\n",
      "Iteration 743, loss = 0.00010443\n",
      "Iteration 744, loss = 0.00010437\n",
      "Iteration 745, loss = 0.00010432\n",
      "Iteration 746, loss = 0.00010416\n",
      "Iteration 747, loss = 0.00010406\n",
      "Iteration 748, loss = 0.00010379\n",
      "Iteration 749, loss = 0.00010368\n",
      "Iteration 750, loss = 0.00010352\n",
      "Iteration 751, loss = 0.00010334\n",
      "Iteration 752, loss = 0.00010340\n",
      "Iteration 753, loss = 0.00010317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 754, loss = 0.00010313\n",
      "Iteration 755, loss = 0.00010314\n",
      "Iteration 756, loss = 0.00010302\n",
      "Iteration 757, loss = 0.00010267\n",
      "Iteration 758, loss = 0.00010283\n",
      "Iteration 759, loss = 0.00010256\n",
      "Iteration 760, loss = 0.00010250\n",
      "Iteration 761, loss = 0.00010236\n",
      "Iteration 762, loss = 0.00010221\n",
      "Iteration 763, loss = 0.00010208\n",
      "Iteration 764, loss = 0.00010204\n",
      "Iteration 765, loss = 0.00010199\n",
      "Iteration 766, loss = 0.00010176\n",
      "Iteration 767, loss = 0.00010167\n",
      "Iteration 768, loss = 0.00010161\n",
      "Iteration 769, loss = 0.00010150\n",
      "Iteration 770, loss = 0.00010142\n",
      "Iteration 771, loss = 0.00010132\n",
      "Iteration 772, loss = 0.00010127\n",
      "Iteration 773, loss = 0.00010116\n",
      "Iteration 774, loss = 0.00010101\n",
      "Iteration 775, loss = 0.00010108\n",
      "Iteration 776, loss = 0.00010113\n",
      "Iteration 777, loss = 0.00010080\n",
      "Iteration 778, loss = 0.00010082\n",
      "Iteration 779, loss = 0.00010064\n",
      "Iteration 780, loss = 0.00010044\n",
      "Iteration 781, loss = 0.00010051\n",
      "Iteration 782, loss = 0.00010031\n",
      "Iteration 783, loss = 0.00010017\n",
      "Iteration 784, loss = 0.00010010\n",
      "Iteration 785, loss = 0.00010008\n",
      "Iteration 786, loss = 0.00009998\n",
      "Iteration 787, loss = 0.00009980\n",
      "Iteration 788, loss = 0.00009981\n",
      "Iteration 789, loss = 0.00009994\n",
      "Iteration 790, loss = 0.00009957\n",
      "Iteration 791, loss = 0.00009957\n",
      "Iteration 792, loss = 0.00009943\n",
      "Iteration 793, loss = 0.00009944\n",
      "Iteration 794, loss = 0.00009923\n",
      "Iteration 795, loss = 0.00009942\n",
      "Iteration 796, loss = 0.00009920\n",
      "Iteration 797, loss = 0.00009898\n",
      "Iteration 798, loss = 0.00009889\n",
      "Iteration 799, loss = 0.00009882\n",
      "Iteration 800, loss = 0.00009874\n",
      "Iteration 801, loss = 0.00009864\n",
      "Iteration 802, loss = 0.00009861\n",
      "Iteration 803, loss = 0.00009851\n",
      "Iteration 804, loss = 0.00009856\n",
      "Iteration 805, loss = 0.00009839\n",
      "Iteration 806, loss = 0.00009834\n",
      "Iteration 807, loss = 0.00009821\n",
      "Iteration 808, loss = 0.00009824\n",
      "Iteration 809, loss = 0.00009806\n",
      "Iteration 810, loss = 0.00009794\n",
      "Iteration 811, loss = 0.00009795\n",
      "Iteration 812, loss = 0.00009788\n",
      "Iteration 813, loss = 0.00009776\n",
      "Iteration 814, loss = 0.00009767\n",
      "Iteration 815, loss = 0.00009770\n",
      "Iteration 816, loss = 0.00009757\n",
      "Iteration 817, loss = 0.00009743\n",
      "Iteration 818, loss = 0.00009750\n",
      "Iteration 819, loss = 0.00009744\n",
      "Iteration 820, loss = 0.00009724\n",
      "Iteration 821, loss = 0.00009732\n",
      "Iteration 822, loss = 0.00009703\n",
      "Iteration 823, loss = 0.00009710\n",
      "Iteration 824, loss = 0.00009693\n",
      "Iteration 825, loss = 0.00009695\n",
      "Iteration 826, loss = 0.00009678\n",
      "Iteration 827, loss = 0.00009674\n",
      "Iteration 828, loss = 0.00009695\n",
      "Iteration 829, loss = 0.00009669\n",
      "Iteration 830, loss = 0.00009654\n",
      "Iteration 831, loss = 0.00009657\n",
      "Iteration 832, loss = 0.00009641\n",
      "Iteration 833, loss = 0.00009638\n",
      "Iteration 834, loss = 0.00009639\n",
      "Iteration 835, loss = 0.00009636\n",
      "Iteration 836, loss = 0.00009634\n",
      "Iteration 837, loss = 0.00009631\n",
      "Iteration 838, loss = 0.00009609\n",
      "Iteration 839, loss = 0.00009603\n",
      "Iteration 840, loss = 0.00009588\n",
      "Iteration 841, loss = 0.00009579\n",
      "Iteration 842, loss = 0.00009578\n",
      "Iteration 843, loss = 0.00009575\n",
      "Iteration 844, loss = 0.00009563\n",
      "Iteration 845, loss = 0.00009554\n",
      "Iteration 846, loss = 0.00009552\n",
      "Iteration 847, loss = 0.00009542\n",
      "Iteration 848, loss = 0.00009536\n",
      "Iteration 849, loss = 0.00009538\n",
      "Iteration 850, loss = 0.00009522\n",
      "Iteration 851, loss = 0.00009534\n",
      "Iteration 852, loss = 0.00009528\n",
      "Iteration 853, loss = 0.00009529\n",
      "Iteration 854, loss = 0.00009497\n",
      "Iteration 855, loss = 0.00009510\n",
      "Iteration 856, loss = 0.00009539\n",
      "Iteration 857, loss = 0.00009482\n",
      "Iteration 858, loss = 0.00009502\n",
      "Iteration 859, loss = 0.00009489\n",
      "Iteration 860, loss = 0.00009464\n",
      "Iteration 861, loss = 0.00009464\n",
      "Iteration 862, loss = 0.00009465\n",
      "Iteration 863, loss = 0.00009456\n",
      "Iteration 864, loss = 0.00009441\n",
      "Iteration 865, loss = 0.00009436\n",
      "Iteration 866, loss = 0.00009431\n",
      "Iteration 867, loss = 0.00009444\n",
      "Iteration 868, loss = 0.00009422\n",
      "Iteration 869, loss = 0.00009433\n",
      "Iteration 870, loss = 0.00009413\n",
      "Iteration 871, loss = 0.00009418\n",
      "Iteration 872, loss = 0.00009398\n",
      "Iteration 873, loss = 0.00009398\n",
      "Iteration 874, loss = 0.00009402\n",
      "Iteration 875, loss = 0.00009385\n",
      "Iteration 876, loss = 0.00009394\n",
      "Iteration 877, loss = 0.00009395\n",
      "Iteration 878, loss = 0.00009369\n",
      "Iteration 879, loss = 0.00009370\n",
      "Iteration 880, loss = 0.00009359\n",
      "Iteration 881, loss = 0.00009355\n",
      "Iteration 882, loss = 0.00009371\n",
      "Iteration 883, loss = 0.00009350\n",
      "Iteration 884, loss = 0.00009340\n",
      "Iteration 885, loss = 0.00009329\n",
      "Iteration 886, loss = 0.00009318\n",
      "Iteration 887, loss = 0.00009322\n",
      "Iteration 888, loss = 0.00009320\n",
      "Iteration 889, loss = 0.00009309\n",
      "Iteration 890, loss = 0.00009309\n",
      "Iteration 891, loss = 0.00009293\n",
      "Iteration 892, loss = 0.00009312\n",
      "Iteration 893, loss = 0.00009303\n",
      "Iteration 894, loss = 0.00009283\n",
      "Iteration 895, loss = 0.00009283\n",
      "Iteration 896, loss = 0.00009310\n",
      "Iteration 897, loss = 0.00009293\n",
      "Iteration 898, loss = 0.00009311\n",
      "Iteration 899, loss = 0.00009261\n",
      "Iteration 900, loss = 0.00009260\n",
      "Iteration 901, loss = 0.00009247\n",
      "Iteration 902, loss = 0.00009251\n",
      "Iteration 903, loss = 0.00009234\n",
      "Iteration 904, loss = 0.00009235\n",
      "Iteration 905, loss = 0.00009234\n",
      "Iteration 906, loss = 0.00009230\n",
      "Iteration 907, loss = 0.00009228\n",
      "Iteration 908, loss = 0.00009224\n",
      "Iteration 909, loss = 0.00009221\n",
      "Iteration 910, loss = 0.00009236\n",
      "Iteration 911, loss = 0.00009207\n",
      "Iteration 912, loss = 0.00009204\n",
      "Iteration 913, loss = 0.00009196\n",
      "Iteration 914, loss = 0.00009182\n",
      "Iteration 915, loss = 0.00009190\n",
      "Iteration 916, loss = 0.00009175\n",
      "Iteration 917, loss = 0.00009171\n",
      "Iteration 918, loss = 0.00009171\n",
      "Iteration 919, loss = 0.00009161\n",
      "Iteration 920, loss = 0.00009162\n",
      "Iteration 921, loss = 0.00009161\n",
      "Iteration 922, loss = 0.00009156\n",
      "Iteration 923, loss = 0.00009145\n",
      "Iteration 924, loss = 0.00009140\n",
      "Iteration 925, loss = 0.00009135\n",
      "Iteration 926, loss = 0.00009133\n",
      "Iteration 927, loss = 0.00009128\n",
      "Iteration 928, loss = 0.00009121\n",
      "Iteration 929, loss = 0.00009116\n",
      "Iteration 930, loss = 0.00009113\n",
      "Iteration 931, loss = 0.00009123\n",
      "Iteration 932, loss = 0.00009120\n",
      "Iteration 933, loss = 0.00009105\n",
      "Iteration 934, loss = 0.00009103\n",
      "Iteration 935, loss = 0.00009101\n",
      "Iteration 936, loss = 0.00009092\n",
      "Iteration 937, loss = 0.00009089\n",
      "Iteration 938, loss = 0.00009076\n",
      "Iteration 939, loss = 0.00009088\n",
      "Iteration 940, loss = 0.00009108\n",
      "Iteration 941, loss = 0.00009066\n",
      "Iteration 942, loss = 0.00009080\n",
      "Iteration 943, loss = 0.00009061\n",
      "Iteration 944, loss = 0.00009058\n",
      "Iteration 945, loss = 0.00009061\n",
      "Iteration 946, loss = 0.00009067\n",
      "Iteration 947, loss = 0.00009040\n",
      "Iteration 948, loss = 0.00009047\n",
      "Iteration 949, loss = 0.00009030\n",
      "Iteration 950, loss = 0.00009040\n",
      "Iteration 951, loss = 0.00009021\n",
      "Iteration 952, loss = 0.00009024\n",
      "Iteration 953, loss = 0.00009018\n",
      "Iteration 954, loss = 0.00009012\n",
      "Iteration 955, loss = 0.00009012\n",
      "Iteration 956, loss = 0.00009006\n",
      "Iteration 957, loss = 0.00009001\n",
      "Iteration 958, loss = 0.00009011\n",
      "Iteration 959, loss = 0.00009000\n",
      "Iteration 960, loss = 0.00008986\n",
      "Iteration 961, loss = 0.00008997\n",
      "Iteration 962, loss = 0.00009016\n",
      "Iteration 963, loss = 0.00008980\n",
      "Iteration 964, loss = 0.00008992\n",
      "Iteration 965, loss = 0.00008969\n",
      "Iteration 966, loss = 0.00008966\n",
      "Iteration 967, loss = 0.00008975\n",
      "Iteration 968, loss = 0.00008981\n",
      "Iteration 969, loss = 0.00008964\n",
      "Iteration 970, loss = 0.00008983\n",
      "Iteration 971, loss = 0.00008960\n",
      "Iteration 972, loss = 0.00008943\n",
      "Iteration 973, loss = 0.00008945\n",
      "Iteration 974, loss = 0.00008951\n",
      "Iteration 975, loss = 0.00008942\n",
      "Iteration 976, loss = 0.00008927\n",
      "Iteration 977, loss = 0.00008941\n",
      "Iteration 978, loss = 0.00008925\n",
      "Iteration 979, loss = 0.00008919\n",
      "Iteration 980, loss = 0.00008923\n",
      "Iteration 981, loss = 0.00008906\n",
      "Iteration 982, loss = 0.00008924\n",
      "Iteration 983, loss = 0.00008907\n",
      "Iteration 984, loss = 0.00008904\n",
      "Iteration 985, loss = 0.00008898\n",
      "Iteration 986, loss = 0.00008901\n",
      "Iteration 987, loss = 0.00008907\n",
      "Iteration 988, loss = 0.00008900\n",
      "Iteration 989, loss = 0.00008877\n",
      "Iteration 990, loss = 0.00008904\n",
      "Iteration 991, loss = 0.00008868\n",
      "Iteration 992, loss = 0.00008871\n",
      "Iteration 993, loss = 0.00008870\n",
      "Iteration 994, loss = 0.00008870\n",
      "Iteration 995, loss = 0.00008857\n",
      "Iteration 996, loss = 0.00008862\n",
      "Iteration 997, loss = 0.00008852\n",
      "Iteration 998, loss = 0.00008854\n",
      "Iteration 999, loss = 0.00008845\n",
      "Iteration 1000, loss = 0.00008843\n"
     ]
    }
   ],
   "source": [
    "nnModelADAM = mlpADAM.fit(trainData, trainLabelE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "train_df= pd.read_csv('train.csv')\n",
    "test_df= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    train_values = train_df.values\n",
    "    test_values = test_df.values\n",
    "    np.random.shuffle(train_values)\n",
    "    np.random.shuffle(test_values)\n",
    "    X_train = train_values[:, :-1]\n",
    "    X_test = test_values[:, :-1]\n",
    "    y_train = train_values[:, -1]\n",
    "    y_test = test_values[:, -1]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698698698698699"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression : 88.48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698698698698699"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "pca= PCA(n_components=200)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8948948948948949"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature scaling between -1 to 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.908908908908909"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_all_data()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest : 90.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#separating features and labels\n",
    "trainData= train.drop('Activity',axis=1).values\n",
    "trainLabel= train.Activity.values\n",
    "\n",
    "testData= test.drop('Activity', axis=1).values\n",
    "testLabel= test.Activity.values\n",
    "\n",
    "#encoding labels\n",
    "from sklearn import preprocessing\n",
    "\n",
    "encoder= preprocessing.LabelEncoder()\n",
    "\n",
    "#encoding test labels\n",
    "encoder.fit(testLabel)\n",
    "testLabelE = encoder.transform(testLabel)\n",
    "\n",
    "#encoding train labels\n",
    "encoder.fit(trainLabel)\n",
    "trainLabelE = encoder.transform(trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy : 0.76476\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=24)\n",
    "\n",
    "knnmodel= clf.fit(trainData, trainLabelE)\n",
    "pred= clf.predict(testData)\n",
    "\n",
    "acc= accuracy_score(testLabelE, pred)\n",
    "print(\"KNN accuracy : %.5f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN accuracy : 0.76476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.766767\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "deTreeClf= DecisionTreeClassifier()\n",
    "tree = deTreeClf.fit(trainData, trainLabelE)\n",
    "testpred = tree.predict(testData)\n",
    "\n",
    "acc1= accuracy_score(testLabelE,testpred)\n",
    "print('Accuracy : %f'% acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Accuracy : 0.757758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set found:\n",
      "{'C': 100, 'kernel': 'rbf'}\n",
      "Detailed grid scores:\n",
      "0.998 (+/-0.005) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 100, 'kernel': 'rbf'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 100, 'kernel': 'poly'}\n",
      "\n",
      "0.978 (+/-0.016) for {'C': 100, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.998 (+/-0.005) for {'C': 50, 'kernel': 'linear'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 50, 'kernel': 'rbf'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 50, 'kernel': 'poly'}\n",
      "\n",
      "0.981 (+/-0.010) for {'C': 50, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.998 (+/-0.005) for {'C': 20, 'kernel': 'linear'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 20, 'kernel': 'rbf'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 20, 'kernel': 'poly'}\n",
      "\n",
      "0.971 (+/-0.004) for {'C': 20, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.998 (+/-0.005) for {'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "0.985 (+/-0.014) for {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.999 (+/-0.004) for {'C': 1, 'kernel': 'poly'}\n",
      "\n",
      "0.685 (+/-0.025) for {'C': 1, 'kernel': 'sigmoid'}\n",
      "\n",
      "0.998 (+/-0.005) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "0.749 (+/-0.017) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "0.966 (+/-0.022) for {'C': 0.1, 'kernel': 'poly'}\n",
      "\n",
      "0.294 (+/-0.041) for {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly','sigmoid'],\n",
    "    'C': [100, 50, 20, 1, 0.1]\n",
    "}\n",
    "\n",
    "selector = GridSearchCV(SVC(), parameters, scoring='accuracy') # we only care about accuracy here\n",
    "selector.fit(trainData, trainLabel)\n",
    "\n",
    "print('Best parameter set found:')\n",
    "print(selector.best_params_)\n",
    "print('Detailed grid scores:')\n",
    "means = selector.cv_results_['mean_test_score']\n",
    "stds = selector.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, selector.cv_results_['params']):\n",
    "    print('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8738738738738738\n"
     ]
    }
   ],
   "source": [
    "clf=SVC(kernel='linear', C=100).fit(trainData, trainLabel)\n",
    "y_pred= clf.predict(testData)\n",
    "print('Accuracy score:',accuracy_score(testLabel,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search : Accuracy score: 87.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification models\n",
    "# logistic regression : 88.48%\n",
    "#Random forest : 90.39\n",
    "#KNN accuracy : 0.76476\n",
    "# Decision Tree Accuracy : 0.757758\n",
    "#Grid search : Accuracy score: 87.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
